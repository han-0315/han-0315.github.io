---
layout: post
title: istio traffic control
date: 2025-04-27 09:00 +0900 
description: istio 트래픽 제어 방식 살펴보기
category: [Kubernetes, Network] 
tags: [istio, CloudNet, Kubernetes, Network, istio#3, traffic, controle] 
pin: false
math: true
mermaid: true
---
istio 트래픽 제어 방식 살펴보기
<!--more-->


### 들어가며


2주차에서는 Envoy의 내부 원리와 외부 트래픽을 클러스터 내부로 가져오는 방식 대해 자세히 살펴봤다. 


여기서는 들어온 트래픽을 세밀하게 제어하는 방법, 트래픽 관리 및 제어에 대해 중점적으로 살펴본다. 그리고 해당 기능이 왜 필요한지도 알아본다.


*아래의 실습과 관련된 그림은 스터디원 이제원님이 직접 그려주셨다. 


## 실습 환경 구성


실습 환경:  **docker** (**kind** - k8s 1.23.17 ‘23.2.28 - [Link](https://kubernetes.io/releases/patch-releases/)) , **istio** 1.17.8(’23.10.11) - [Link](https://istio.io/latest/news/releases/1.17.x/)


환경은 최신버전이 아닌데, 그 이유는 Istio in Action의 번역판이 과거 개정판의 내용을 담고 있고 이를 맞추기 위함이다.


### kind


실습 환경으로는 kind(kubernetes in docker)를 사용한다. kind를 통해 로컬에서 쿠버네티스와 가장 유사한 환경을 구축할 수 있다는 장점이 있다. 


#### kind 설치(macOS)


윈도우 환경이라면, [문서](https://kind.sigs.k8s.io/docs/user/quick-start/#installation)를 참고


```bash
brew install kind
```


#### 클러스터 구성


단일 노드 클러스터를 구성한다. 


```bash
kind create cluster --name myk8s --image kindest/node:v1.23.17 --config - <<EOF
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  extraPortMappings:
  - containerPort: 30000 # Sample Application (istio-ingrssgateway) HTTP
    hostPort: 30000
  - containerPort: 30001 # Prometheus
    hostPort: 30001
  - containerPort: 30002 # Grafana
    hostPort: 30002
  - containerPort: 30003 # Kiali
    hostPort: 30003
  - containerPort: 30004 # Tracing
    hostPort: 30004
  - containerPort: 30005 # Sample Application (istio-ingrssgateway) HTTPS
    hostPort: 30005
  - containerPort: 30006 # TCP Route
    hostPort: 30006
  - containerPort: 30007 # kube-ops-view
    hostPort: 30007
  _extraMounts: 
  - hostPath: $(pwd)
    containerPath: /istiobook_
networking:
  podSubnet: 10.10.0.0/16
  serviceSubnet: 10.200.1.0/24
EOF

```


노드 컨테이너에 실습에 필요한 유틸리티를 설치한다.


```bash
docker exec -it myk8s-control-plane sh -c 'apt update && apt install tree psmisc lsof wget bridge-utils net-tools dnsutils tcpdump ngrep iputils-ping git vim -y'
```


### istio


istio를 설치하기 앞서, 우선 노드에 접속한다.


```bash
docker exec -it **myk8s****-control-plane bash**
```


#### istioctl 설치


```bash
export ISTIOV=1.17.8
echo 'export ISTIOV=1.17.8' >> /root/.bashrc
curl -s -L https://istio.io/downloadIstio | ISTIO_VERSION=$ISTIOV sh -
cp istio-$ISTIOV/bin/istioctl /usr/local/bin/istioctl
```


아래의 명령어로 정상 설치여부를 확인할 수 있다.


```bash
istioctl version --remote=false
1.17.8
```


#### istio 배포


```bash
istioctl install --set profile=default -y
```


## Deployment vs Release


새로운 버전을 배포할 때, 기존 버전의 리소스와 새로운 버전의 리소스가 공존하는 상황이 온다. Istio에서는 이런 경우 트래픽을 섬세하게 제어할 수 있다. 운영 환경에 배포할 때는 새 코드를 운영 환경 리소스(서버, 컨테이너 등)에 배포하지만 트래픽을 라우팅하지 않는다. 내부 직원들만 새로운 기능 리소스에 접근하고 싶을 때, istio에서는 어떤 설정이 필요할지 아래에서 알아본다.


#### 용어


**Deployment**는 새로운 기능/버전이 들어간 리소스를 의미한다. **Release**는 사용자에게 새로운 버전을 노출하는 것을 의미하고 여기서는 직접 트래픽이 해당 Deployment에게 전달되는 경우를 의미한다. 


![image.png](/assets/img/post/트래픽%20제어/1.png)


출처: [https://www.sourcedgroup.com/blog/canary-release-and-deployment-on-istio-service-mesh/](https://www.sourcedgroup.com/blog/canary-release-and-deployment-on-istio-service-mesh/)


## Traffic routing


위와 같이 실제 운영환경에서 새로운 버전을 배포하고, **이를 내부 직원들만 접근**할 수 있도록 istio를 설정해보는 실습을 진행한다. 


![image.png](/assets/img/post/트래픽%20제어/2.png)


### **catalog 서비스의 v1** 배포(기존)


기존의 catalog 서비스를 배포한다.


```bash
kubectl apply -f services/catalog/kubernetes/catalog.yaml -n istioinaction
```


/etc/hosts 설정을 통해 테스트 도메인을 등록한다.


```bash
echo "127.0.0.1       catalog.istioinaction.io" | sudo tee -a /etc/hosts
```


#### istio 설정


Gateway, Virtual Service 설정을 통해 트래픽이 잘 들어올 수 있도록 설정한다.


```yaml
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: catalog-gateway
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - "catalog.istioinaction.io"
```


```yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: catalog-vs-from-gw
spec:
  hosts:
  - "catalog.istioinaction.io"
  gateways:
  - catalog-gateway
  http:
  - route:
    - destination:
        host: catalog
```


```bash
kubectl get gw,vs -n istioinaction

NAME                                          AGE
gateway.networking.istio.io/catalog-gateway   14m

NAME                                                    GATEWAYS              HOSTS                          AGE
virtualservice.networking.istio.io/catalog-vs-from-gw   ["catalog-gateway"]   ["catalog.istioinaction.io"]   14m
```



[http://catalog.istioinaction.io:30000/items](http://catalog.istioinaction.io:30000/items) 경로에 접근하면, 배포한 catalog v1 버전의 items를 직접 확인할 수 있다.


![image.png](/assets/img/post/트래픽%20제어/3.png)


이제 지표 수집을 위해 반복적으로 catalog 도메인에 정보를 읽어온다.


```bash
while true; do curl -s http://catalog.istioinaction.io:30000/items/ ; sleep 1; echo; done
```


### **catalog 서비스의 v2** 배포(신규)


이제 새로운 기능 역할을 하는 v2 리소스를 배포한다.


```bash
kubectl apply -f services/catalog/kubernetes/catalog-deployment-v2.yaml -n istioinaction
```


Label을 보면, `version = v1 or v2` 로 구분되는 것을 확인할 수 있다.


```bash
kubectl get deploy -n istioinaction --show-labels

NAME         READY   UP-TO-DATE   AVAILABLE   AGE    LABELS
catalog      1/1     1            1           20m    app=catalog,version=v1
catalog-v2   1/1     1            1           3m6s   app=catalog,version=v2
```


관련 파드의 IP 확인


```bash
kubectl get pod -n istioinaction -o wide

NAME                          READY   STATUS    RESTARTS   AGE     IP           NODE                  NOMINATED NODE   READINESS GATES
catalog-6cf4b97d-p8h4p        2/2     Running   0          20m     10.10.0.12   myk8s-control-plane   <none>           <none>
catalog-v2-6df885b555-7fjqf   2/2     Running   0          3m19s   10.10.0.13   myk8s-control-plane   <none>           <none>
```


이제 kiali를 통해 트래픽을 확인하면, 최근 5분 기준으로 하여 아직 v2로 많은 트래픽이 흐르지 않았지만 공평하게 분산되는 것을 알 수 있다.


![image.png](/assets/img/post/트래픽%20제어/4.png)


### 새로운 v2로 일반 사용자의 트래픽이 흘러들어가지 않도록 설정


이제 새로운 deployment를 일반 사용자가 접근하지 않도록 **DestinationRule**을 설정해본다.


```yaml
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: catalog
spec:
  host: catalog.istioinaction.svc.cluster.local
  subsets:
  - name: version-v1
    labels:
      version: v1
  - name: version-v2
    labels:
      version: v2
```


istio proxy 설정을 확인하면, 아래와 같이 version1, version2 서브넷이 업데이트 된 것을 볼 수 있다.


```bash
docker exec -it myk8s-control-plane istioctl proxy-config cluster deploy/istio-ingressgateway.istio-system --fqdn catalog.istioinaction.svc.cluster.local

SERVICE FQDN                                PORT     SUBSET         DIRECTION     TYPE     DESTINATION RULE
catalog.istioinaction.svc.cluster.local     80       -              outbound      EDS      catalog.istioinaction
catalog.istioinaction.svc.cluster.local     80       version-v1     outbound      EDS      catalog.istioinaction
catalog.istioinaction.svc.cluster.local     80       version-v2     outbound      EDS      catalog.istioinaction
```


별도로 proxy 엔드포인트를 확인해봐도 동일하다.


```bash
docker exec -it myk8s-control-plane istioctl proxy-config endpoint deploy/istio-ingressgateway.istio-system | egrep 'ENDPOINT|istioinaction'

ENDPOINT                                                STATUS      OUTLIER CHECK     CLUSTER
10.10.0.12:3000                                         HEALTHY     OK                outbound|80|version-v1|catalog.istioinactio .svc.cluster.local
10.10.0.12:3000                                         HEALTHY     OK                outbound|80||catalog.istioinaction.svc.cluster.local
10.10.0.13:3000                                         HEALTHY     OK                outbound|80|version-v2|catalog.istioinactio .svc.cluster.local
10.10.0.13:3000                                         HEALTHY     OK                outbound|80||catalog.istioinaction.svc.cluster.local
```


이제 Virtual Service를 업데이트하여, 트래픽이 v1으로만 가게끔 설정한다.


```yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: catalog-vs-from-gw
spec:
  hosts:
  - "catalog.istioinaction.io"
  gateways:
  - catalog-gateway
  http:
  - route:
    - destination:
        host: catalog
        subset: version-v1
```


라우팅 정보를 확인하면, `"outbound|80|version-v1|catalog.istioinaction.svc.cluster.local"`으로 version-v1 서브넷으로만 라우팅되는 것을 볼 수 있다.


```bash
docker exec -it myk8s-control-plane istioctl proxy-config routes deploy/istio-ingressgateway.istio-system --name http.8080 -o json

[
    {
        "name": "http.8080",
        "virtualHosts": [
            {
                "name": "catalog.istioinaction.io:80",
                "domains": [
                    "catalog.istioinaction.io"
                ],
                "routes": [
                    {
                        "match": {
                            "prefix": "/"
                        },
                        "route": {
                            "cluster": "outbound|80|version-v1|catalog.istioinaction.svc.cluster.local",
                            "timeout": "0s",
                            ...
                        },
                        ...
    }
]
```


실제 proxy config도 확인해보면 아래와 같이 1개의(v1) 엔드포인트만 볼 수 있다.


```bash
docker exec -it myk8s-control-plane istioctl proxy-config endpoint deploy/istio-ingressgateway.istio-system --cluster 'outbound|80|version-v1|catalog.istioinaction.svc.cluster.local'

ENDPOINT            STATUS      OUTLIER CHECK     CLUSTER
10.10.0.12:3000     HEALTHY     OK                outbound|80|version-v1|catalog.istioinaction.svc.cluster.local
```


아래는 kiali 모습


![image.png](/assets/img/post/트래픽%20제어/5.png)


### 내부 직원(특정 사용자)의 트래픽은 v2로 라우팅 설정


이제 테스트를 진행할 **특정 사용자(= 내부 직원)는 헤더를 통해 v2에 접근**할 수 있도록 라우팅 설정을 진행해보자.


Virtual Service를 아래와 같이 수정한다. `x-istio-cohort` 헤더의 값이 `internal` 인 경우 v2로 라우팅한다는 설정이다.


```yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: catalog-vs-from-gw
spec:
  hosts:
  - "catalog.istioinaction.io"
  gateways:
  - catalog-gateway
  http:
  - match:
    - headers:
        x-istio-cohort:
          exact: "internal"
    route:
    - destination:
        host: catalog
        subset: version-v2
  - route:
    - destination:
        host: catalog
        subset: version-v1
```


설정을 진행하고 해당 헤더를 포함하여 [http://catalog.istioinaction.io](http://catalog.istioinaction.io/)에 반복적으로 접근해보자.


```bash
while true; do curl -s http://catalog.istioinaction.io:30000/items/ -H "x-istio-cohort: internal" -I | head -n 1 ; date "+%Y-%m-%d %H:%M:%S" ; sleep 2; echo; done
```


사진에서 볼 수 있듯이 다시 v2로 일부 트래픽(특정 헤더를 가진)이 흘러들어가는 것을 볼 수 있다.


![image.png](/assets/img/post/트래픽%20제어/6.png)


관련 Ingress 설정을 확인해보면, 아래와 같이 이전과 비교되게 라우팅 규칙에 엔드포인트 1개 더 추가되었다.

- `"cluster": "outbound|80|version-v2|catalog.istioinaction.svc.cluster.local"`
- `"cluster": "outbound|80|version-v1|catalog.istioinaction.svc.cluster.local"`

```yaml
docker exec -it myk8s-control-plane istioctl proxy-config routes deploy/istio-ingressgateway.istio-system --name http.8080 -o json

[
    {
        "name": "http.8080",
        "virtualHosts": [
            {
                "name": "catalog.istioinaction.io:80",
                "domains": [
                    "catalog.istioinaction.io"
                ],
                "routes": [
                    {
                        "match": {
                            "prefix": "/",
                            "caseSensitive": true,
                            "headers": [
                                {
                                    "name": "x-istio-cohort",
                                    "stringMatch": {
                                        "exact": "internal"
                                    }
                                }
                            ]
                        },
                        "route": {
                            "cluster": "outbound|80|version-v2|catalog.istioinaction.svc.cluster.local",
                            "timeout": "0s",
                            ...
                        },
                       ...
                    },
                    {
                        "match": {
                            "prefix": "/"
                        },
                        "route": {
                            "cluster": "outbound|80|version-v1|catalog.istioinaction.svc.cluster.local",
                            "timeout": "0s",
                            ...
                        },
                       ...
]
```


catalog의 proxy 설정을 확인하면 catalog 서비스 IP만 존재하는 것을 볼 수 있다. 이는 ingress로 들어올 때 트래픽을 제어하기에 아래는 다음과 같이 별도 구분이 없다.


```bash
docker exec -it myk8s-control-plane istioctl proxy-config routes deploy/catalog.istioinaction --name 80 -o json

[
    {
        "name": "80",
        "virtualHosts": [
            {
                "name": "catalog.istioinaction.svc.cluster.local:80",
                "domains": [
                    "catalog.istioinaction.svc.cluster.local",
                    "catalog",
                    "catalog.istioinaction.svc",
                    "catalog.istioinaction",
                    "10.200.1.92"
                ],
                "routes": [
                    {
                        "name": "default",
                        "match": {
                            "prefix": "/"
                        },
                        "route": {
                            "cluster": "outbound|80||catalog.istioinaction.svc.cluster.local",
                            "timeout": "0s",
                            "retryPolicy": {
                                "retryOn": "connect-failure,refused-stream,unavailable,cancelled,retriable-status-codes",
                                "numRetries": 2,
                                "retryHostPredicate": [
                                    {
                                        "name": "envoy.retry_host_predicates.previous_hosts",
                                        "typedConfig": {
                                            "@type": "type.googleapis.com/envoy.extensions.retry.host.previous_hosts.v3.PreviousHostsPredicate"
                                        }
                                    }
                                ],
                                "hostSelectionRetryMaxAttempts": "5",
                                "retriableStatusCodes": [
                                    503
                                ]
                            },
                            "maxGrpcTimeout": "0s"
                        },
                        "decorator": {
                            "operation": "catalog.istioinaction.svc.cluster.local:80/*"
                        }
                    }
                ],
                "includeRequestAttemptCount": true
            },
            ...
]
```


### 클러스터 내부 라우팅 제어


위에서는 외부에서 해당 catalog 서비스로 직접 들어오는 트래픽을 제어했다. 여기서는 **클러스터 내부 통신에 대한 제어**를 진행해본다. 아래의 그림과 같이 catalog 앞에 webapp이라는 서비스를 두고, webapp에서 catalog로 들어오는 트래픽을 제어한다.


![image.png](/assets/img/post/트래픽%20제어/7.png)


#### webapp 배포 및 라우팅 설정


```bash
kubectl apply -n istioinaction -f services/webapp/kubernetes/webapp.yaml
```


아래와 같이 istio 리소스를 배포하여 [webapp.istioinaction.io](http://webapp.istioinaction.io/) 경로로 들어오는 트래픽이 정상적으로 webapp 파드로 라우팅되게 한다.


```yaml
---
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: coolstore-gateway
spec:
  selector:
    istio: ingressgateway # use istio default controller
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - "webapp.istioinaction.io"
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: webapp-virtualservice
spec:
  hosts:
  - "webapp.istioinaction.io"
  gateways:
  - coolstore-gateway
  http:
  - route:
    - destination:
        host: webapp
        port:
          number: 80
```


배포 후 [webapp.istioinaction.io](http://webapp.istioinaction.io/) 사이트에 접속하면 아래와 같은 화면을 볼 수 있다.


![image.png](/assets/img/post/트래픽%20제어/8.png)


이제 자세한 과정을 보기 위해 webapp 로그를 활성화한다.


```bash
cat << EOF | kubectl apply -f -
apiVersion: telemetry.istio.io/v1alpha1
kind: Telemetry
metadata:
  name: webapp
  namespace: istioinaction
spec:
  selector:
    matchLabels:
      app: webapp
  accessLogging:
  - providers:
    - name: envoy #2 액세스 로그를 위한 프로바이더 설정
    disabled: false #3 disable 를 false 로 설정해 활성화한다
EOF

```


kiali 그림으로 들어오는 트래픽을 확인해보면, webapp → catalog(v1,v2)로 공평하게 분산되는 것을 볼 수 있다.


![image.png](/assets/img/post/트래픽%20제어/9.png)


#### 트래픽 제어


이제 webapp에서 catalog로 들어오는 트래픽을 제어해본다. (header를 통한 v1, v2 제어)


동일하게 DestinationRule을 배포하고,


```bash
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: catalog
spec:
  host: catalog.istioinaction.svc.cluster.local
  subsets:
  - name: version-v1
    labels:
      version: v1
  - name: version-v2
    labels:
      version: v2
```


Virtual Service에 관련 설정을 둔다. 이전과는 차이가 gateways mesh 설정이 들어간 것을 볼 수 있다. 


```bash
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: catalog
spec:
  hosts:
  - catalog
  gateways: 
    - mesh  
  http:
  - route:
    - destination:
        host: catalog
        subset: version-v1
```


Virtual Service를 조회해보면, 방금 배포한 catalog 서비스를 확인할 수 있다.


```bash
kubectl get vs -n istioinaction

NAME                    GATEWAYS                HOSTS                         AGE
catalog                 ["mesh"]                ["catalog"]                   16s
webapp-virtualservice   ["coolstore-gateway"]   ["webapp.istioinaction.io"]   3m49s
```


이제 라우팅 규칙을 확인해보자. 위에서 살펴봤을 땐 catalog.istioinaction.svc로만 라우팅되는 것(v1,v2 분산)을 볼 수 있었지만 지금은 v1으로만 향하는 것을 볼 수 있다.

- `outbound|80|version-v1|catalog.istioinaction.svc.cluster.local`

```bash
cat webapp-routes.json | jq

[
  {
    "name": "80",
    "virtualHosts": [
      {
        "name": "catalog.istioinaction.svc.cluster.local:80",
        "domains": [
          "catalog.istioinaction.svc.cluster.local",
          "catalog",
          "catalog.istioinaction.svc",
          "catalog.istioinaction",
          "10.200.1.92"
        ],
        "routes": [
          {
            "match": {
              "prefix": "/"
            },
            "route": {
              "cluster": "outbound|80|version-v1|catalog.istioinaction.svc.cluster.local",
              "timeout": "0s",
              "...
        ],
      },
      ...
```


![image.png](/assets/img/post/트래픽%20제어/10.png)


## **Traffic Shifting**


**Traffic Shifting**은 Istio에서 트래픽을 여러 서비스 버전으로 비율을 나눠서 보내는 기능을 말한다. 주로 카나리 배포와 같은 점진적인 배포 상황에서 적용한다. 예를 들어, 전체 요청 중 90%는 기존 버전(v1)으로, 10%는 새 버전(v2)으로 보내어 리스크를 최소한으로 하여 신규 배포가 문제가 없는지 확인할 수 있다.


만약, 문제가 생긴다면 다시 v1으로만 트래픽이 향하도록 빠르게 롤백할 수 있다.


지표를 깔끔하게 보기 위해 v1으로만 라우팅 되도록 설정한다.


```yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: catalog
spec:
  hosts:
  - catalog
  gateways:
    - mesh
  http:
  - route:
    - destination:
        host: catalog
        subset: version-v1
        
```


이제 VirtualService에서 weight를 조정한다. v1:v2 = 9:1 비중으로 조절해본다.


```yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: catalog
spec:
  hosts:
  - catalog
  gateways:
  - mesh
  http:
  - route:
    - destination:
        host: catalog
        subset: version-v1
      weight: 90
    - destination:
        host: catalog
        subset: version-v2
      weight: 10 
```


Virtual Service가 반영된 것을 확인하고


```yaml
kubectl get vs -n istioinaction catalog

NAME      GATEWAYS   HOSTS         AGE
catalog   ["mesh"]   ["catalog"]   11m
```


100번 접속하고, 이중에 v2로 라우팅된 트래픽의 횟수를 파악해보면 딱 10(9:1)에 맞게 나온다.


```yaml
for i in {1..100}; do curl -s http://webapp.istioinaction.io:30000/api/catalog | grep -i imageUrl ; done | wc -l

      10
```


kiali로 확인해도 트래픽 비중이 9:1로 알맞게 shifting된 것을 볼 수 있다.


![image.png](/assets/img/post/트래픽%20제어/11.png)


## Traffic mirroring


기존 서비스로 트래픽을 정상적으로 보내면서, **동시에** 복제된 트래픽을 새로운 서비스(또는 버전)로 보내는 방법이다. 미러링된 트래픽은 실제 응답에 영향을 주지 않아 **“사용자에게 영향없이”** 새 버전을 검증할 수 있다.


![image.png](/assets/img/post/트래픽%20제어/12.png)


Virtual Service에 mirror 옵션을 추가한다.


```yaml
kind: VirtualService
metadata:
  name: catalog
spec:
  hosts:
  - catalog
  gateways:
    - mesh
  http:
  - route:
    - destination:
        host: catalog
        subset: version-v1
      weight: 100
    mirror:
      host: catalog
      subset: version-v2
```


이제 배포 후 로그를 확인해보면


`catalog.istioinaction-shadow` -shadow가 붙어 미머링된 요청임을 식별할 수 있다. 


```yaml
 kubectl logs -n istioinaction -l app=catalog -l version=v2 -c catalog -f
request path: /items
blowups: {}
number of blowups: 0
GET catalog.istioinaction-shadow:80 /items 200 698 - 0.513 ms
GET /items 200 0.513 ms - 698
request path: /items
blowups: {}
number of blowups: 0
```


추가적으로 아래의 config에서도 `"requestMirrorPolicies"`를 확인할 수 있다.


```json
[
  {
    "name": "80",
    "virtualHosts": [
      {
        "name": "catalog.istioinaction.svc.cluster.local:80",
        "domains": [
          "catalog.istioinaction.svc.cluster.local",
          "catalog",
          "catalog.istioinaction.svc",
          "catalog.istioinaction",
          "10.200.1.92"
        ],
        "routes": [
          {
            "match": {
              "prefix": "/"
            },
            "route": {
              "cluster": "outbound|80|version-v1|catalog.istioinaction.svc.cluster.local",
              "timeout": "0s",
              ...
              "requestMirrorPolicies": [
                {
                  "cluster": "outbound|80|version-v2|catalog.istioinaction.svc.cluster.local",
                  "runtimeFraction": {
                    "defaultValue": {
                      "numerator": 100
                    }
                  },
                  "traceSampled": false
                }
              ],
              "maxGrpcTimeout": "0s"
            },
            "metadata": {
              "filterMetadata": {
                "istio": {
                  "config": "/apis/networking.istio.io/v1alpha3/namespaces/istioinaction/virtual-service/catalog"
                }
              }
            },
            "decorator": {
              "operation": "catalog.istioinaction.svc.cluster.local:80/*"
            }
          }
        ],
        "includeRequestAttemptCount": true
      },
      ...
```


## ServiceEntry


만약, 특정 서비스의 트래픽이 외부로 나가면 안되는 경우라면 예를 들어Database의 클러스터 외부에서 직접 요청하지않는다. 이럴 땐 해당 서비스가 외부와 직접 통신하는 것에 막을 수 있다.


### 외부 트래픽 차단하기


`outboundTrafficPolicy.mode` 설정을 `REGISTRY_ONLY` 로 변경한다.


```bash
istioctl install --set profile=default --set meshConfig.outboundTrafficPolicy.mode=REGISTRY_ONLY
```


아래와 같이 operator를 확인하여 설정이 적용된 것을 볼 수 있다.


```bash
 kubectl get istiooperators -n istio-system -o json | grep -A2 outbound
 "outboundTrafficPolicy": {
                        "mode": "REGISTRY_ONLY"
                    }
```


이제 webapp에서 외부 데이터를 가져오려고 하면, 아래와 같이 fail이 발생하는 것을 볼 수 있다.


```bash
kubectl exec -it deploy/webapp -n istioinaction -c webapp -- wget https://raw.githubusercontent.com/gasida/KANS/refs/heads/main/msa/sock-shop-demo.yaml

Connecting to raw.githubusercontent.com (185.199.111.133:443)
wget: error getting response: Connection reset by peer
command terminated with exit code 1
```


![image.png](/assets/img/post/트래픽%20제어/13.png)


### ServiceEntry를 이용해 외부 서비스 연결하기


외부와 통신을 막은 상태에서도 Sentry를 통해 클러스터 외부 서비스와 연결할 수 있다.


우선 forum을 배포한다.


```bash
kubectl apply -f services/forum/kubernetes/forum-all.yaml -n istioinaction
```


![image.png](/assets/img/post/트래픽%20제어/14.png)


이후 ServiceEntry 설정을 두어 jsonplaceholder.typicode.com을 사용하는 서비스와의 통신을 허용한다.


```bash
apiVersion: networking.istio.io/v1alpha3
kind: ServiceEntry
metadata:
  name: jsonplaceholder
spec:
  hosts:
  - jsonplaceholder.typicode.com
  ports:
  - number: 80
    name: http
    protocol: HTTP
  resolution: DNS
  location: MESH_EXTERNAL
```


해당 호스트(jsonplaceholder.typicode.com)의 엔드포인트를 확인가능하고,


```bash
docker exec -it myk8s-control-plane istioctl proxy-config endpoint deploy/forum.istioinaction --cluster 'outbound|80||jsonplaceholder.typicode.com'

ENDPOINT            STATUS      OUTLIER CHECK     CLUSTER
104.21.112.1:80     HEALTHY     OK                outbound|80||jsonplaceholder.typicode.com
104.21.16.1:80      HEALTHY     OK                outbound|80||jsonplaceholder.typicode.com
104.21.32.1:80      HEALTHY     OK                outbound|80||jsonplaceholder.typicode.com
104.21.48.1:80      HEALTHY     OK                outbound|80||jsonplaceholder.typicode.com
104.21.64.1:80      HEALTHY     OK                outbound|80||jsonplaceholder.typicode.com
104.21.80.1:80      HEALTHY     OK                outbound|80||jsonplaceholder.typicode.com
104.21.96.1:80      HEALTHY     OK                outbound|80||jsonplaceholder.typicode.com
```


실제 webapp 사이트에도 접속하면 하단의 오른쪽과 같이 외부 서비스(Forum)의 데이터를 가져오는 것을 볼 수 있다.


![image.png](/assets/img/post/트래픽%20제어/15.png)


마지막으로 kiali에서도 확인가능하다.


![image.png](/assets/img/post/트래픽%20제어/16.png)


## 서킷 브레이커


서킷 브레이커는 비정상적인 서비스로 인해 전체 시스템이 붕괴되지 않도록 해당 엔드포인트를 차단하는 기능이다. 


Istio에서도 서킷브레이커 기능을 제공한다. 관련 설정은 `Destination Rule`에서 진행한다.


#### 환경 구성


Tracing 샘플링을 기본 100%로 설정하기(default 1%)


```bash
kubectl describe cm -n istio-system istio | grep -A2 tracing 
  tracing:
    sampling: 100
    zipkin:
```


Delay 1초가 발생하는 simple-backend 배포(시나리오 상, 비정상적인 서비스 역할을 담당한다.)


```bash
kubectl apply -f ch6/simple-backend-delayed.yaml -n istioinaction
```


50%의 확률로 delay 1초가 되도록 환경설정 진행


```bash
kubectl exec -it deploy/simple-backend-1 -n istioinaction -- sh
export TIMING_50_PERCENTILE=1000ms
```


로드테스트 진행


```bash
fortio load -quiet -jitter -t 30s -c 1 -qps 1 http://
simple-web.istioinaction.io:30000
...
WARNING 58.62% of sleep were falling behind
Aggregated Function Time : count 30 avg 1.031874 +/- 0.00898 min 1.015954 max 1.05456325 sum 30.956219
# target 50% 1.03459
# target 75% 1.04458
# target 90% 1.05057
# target 99% 1.05416
# target 99.9% 1.05452
...
Code 200 : 30 (100.0 %)
All done 30 calls (plus 1 warmup) 1031.874 ms avg, 1.0 qps
```


### Connection Pool


istio에서 제어 가능한 설정값은 아래와 같다.

- **maxConnections**`(default 2^32-1)`: 총 커넥션의 수, 로드밸런싱 풀의 엔드포인트 개수에 설정값을 더한 숫자이다.
	- 관련 envoy 지표: `cx_overflow`(_cluster’s connection circuit breaker overflowed__)_
- **http1MaxPendingRequests**`(default 1024)`: 대기 중인 요청, 커넥션이 없어 보류 중인 요청을 어느 정도까지 허용할지 여부이다.
	- 관련 envoy 지표: `upstream_rq_pending_overflow`
- **http2MaxRequests**`(default 1024)`: 모든 호스트에 대한 최대 동시 요청 개수

위의 제어값을 통해 서킷 브레이커가 발동하여 통신이 실패되면 istio는 요청에 대한 응답에 `x-envoy-overloaded` 헤더를 추가한다.


connection Pool을 아래와 같이  max를 1개로 설정한다.


```yaml
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: simple-backend-dr
spec:
  host: simple-backend.istioinaction.svc.cluster.local
  trafficPolicy:
    connectionPool:
      tcp:
        maxConnections: 1 # 커넥션 총 개수 Total number of connections
      http:
        http1MaxPendingRequests: 1 # 대기 중인 요청 Queued requests
        maxRequestsPerConnection: 1 # 커넥션당 요청 개수 Requests per connection
        maxRetries: 1 # Maximum number of retries that can be outstanding to all hosts in a cluster at a given time.
        http2MaxRequests: 1 # 모든 호스트에 대한 최대 동시 요청 개수 Maximum concurrent 
```


배포 후 Proxy Config를 확인해보면, 아래와 같이 `circuitBreakers`의 `thresholds`값이 달라진 것을 확인할 수 있다.


```bash
docker exec -it myk8s-control-plane istioctl proxy-config cluster deploy/simple-backend-1.istioinaction --fqdn simple-backend.istioinaction.svc.cluster.local -o json

[
    {
        "name": "outbound|80||simple-backend.istioinaction.svc.cluster.local",
        "type": "EDS",
        "edsClusterConfig": {
            "edsConfig": {
                "ads": {},
                "initialFetchTimeout": "0s",
                "resourceApiVersion": "V3"
            },
            "serviceName": "outbound|80||simple-backend.istioinaction.svc.cluster.local"
        },
        "connectTimeout": "10s",
        "lbPolicy": "LEAST_REQUEST",
        "circuitBreakers": {
            "thresholds": [
                {
                    "maxConnections": 1,
                    "maxPendingRequests": 1,
                    "maxRequests": 1,
                    "maxRetries": 1,
                    "trackRemaining": true
                }
            ]
        },
        "typedExtensionProtocolOptions": {
            "envoy.extensions.upstreams.http.v3.HttpProtocolOptions": {
                "@type": "type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions",
                "commonHttpProtocolOptions": {
                    "maxRequestsPerConnection": 1
                },
                "explicitHttpConfig": {
                    "httpProtocolOptions": {}
                }
            }
        },
        "commonLbConfig": {
            "localityWeightedLbConfig": {}
        },
        ...
]
```


이제 테스트를 진행해보자.


1) 커넥션 하나로 부하테스트(모두 성공해야 정상이다.)


2) 커넥션 개수를 2개 이상으로 늘려 부하테스트(서킷브레이커 작동 확인)


커넥션 하나로 30초동안 요청을 보내는 부하테스트를 진행한다. 결과는 `Code 200 : 30 (100.0 %)` 으로 모두 성공한 것을 볼 수 있다.


```bash
fortio load -quiet -jitter -t 30s -c 1 -qps 1 --allow-initial-errors http://simple-web.istioinaction.io:30000
Fortio 1.69.4 running at 1 queries per second, 10->10 procs, for 30s: http://simple-web.istioinaction.io:30000
...
WARNING 62.07% of sleep were falling behind
Aggregated Function Time : count 30 avg 1.0319062 +/- 0.01253 min 1.017999667 max 1.080684291 sum 30.957185
# target 50% 1.04826
# target 75% 1.06447
# target 90% 1.0742
# target 99% 1.08004
# target 99.9% 1.08062
Error cases : count 0 avg 0 +/- 0 min 0 max 0 sum 0
# Socket and IP used for each connection:
[0]   1 socket used, resolved to 127.0.0.1:30000, connection timing : count 1 avg 0.000227708 +/- 0 min 0.000227708 max 0.000227708 sum 0.000227708
Sockets used: 1 (for perfect keepalive, would be 1)
Uniform: false, Jitter: true, Catchup allowed: true
IP addresses distribution:
127.0.0.1:30000: 1
Code 200 : 30 (100.0 %)
All done 30 calls (plus 1 warmup) 1031.906 ms avg, 1.0 qps
```


이제 정확한 확인을 위해 통계 수집을 활성화하고, istio container를 reset한다.


```bash
kubectl exec -it deploy/simple-web -c istio-proxy -n istioinaction \
-- curl -X POST localhost:15000/reset_counters
```


관련 overflow 상태값 확인(현재는 0으로 모두 초기화된 것을 확인할 수 있다.)


```bash
kubectl exec -it deploy/simple-web -c istio-proxy -n istioinaction \
 -- curl localhost:15000/stats | grep simple-backend | grep overflow
cluster.outbound|80||simple-backend.istioinaction.svc.cluster.local.upstream_cx_overflow: 0
cluster.outbound|80||simple-backend.istioinaction.svc.cluster.local.upstream_cx_pool_overflow: 0
cluster.outbound|80||simple-backend.istioinaction.svc.cluster.local.upstream_rq_pending_overflow: 0
cluster.outbound|80||simple-backend.istioinaction.svc.cluster.local.upstream_rq_retry_overflow: 0
```


#### 서킷브레이커 확인


이제 커넥션을 2개로 늘려서 부하테스트를 진행한다.


총 58개의 요청 중 `Code 500 : 26 (44.8 %)` 26개가 실패했다. 이는 우리가 http2MaxRequests를 1개로 설정하였기에 예상된 결과였다.


```bash
fortio load -quiet -jitter -t 30s -c 2 -qps 2 --allow-initial-errors http://simple-web.istioinaction.io:30000

Fortio 1.69.4 running at 2 queries per second, 10->10 procs, for 30s: http://simple-web.istioinaction.io:30000
Aggregated Sleep Time : count 57 avg -1.3000824 +/- 1.243 min -4.034061105 max 1.088205728 sum -74.1046987
# range, mid point, percentile, count
>= -4.03406 <= -0.001 , -2.01753 , 80.70, 46
> 0 <= 0.001 , 0.0005 , 82.46, 1
> 0.034 <= 0.039 , 0.0365 , 84.21, 1
> 0.069 <= 0.079 , 0.074 , 85.96, 1
> 0.079 <= 0.089 , 0.084 , 87.72, 1
> 0.119 <= 0.139 , 0.129 , 89.47, 1
> 0.159 <= 0.179 , 0.169 , 91.23, 1
> 0.179 <= 0.199 , 0.189 , 92.98, 1
> 0.899 <= 0.999 , 0.949 , 96.49, 2
> 0.999 <= 1.08821 , 1.0436 , 100.00, 2
# target 50% -1.56941
WARNING 80.70% of sleep were falling behind
Aggregated Function Time : count 58 avg 1.015301 +/- 0.9284 min 0.006459208 max 2.079278667 sum 58.8874607
# target 50% 1.28571
# target 75% 2.01878
# target 90% 2.05508
# target 99% 2.07686
# target 99.9% 2.07904
Error cases : count 26 avg 0.052352668 +/- 0.1948 min 0.006459208 max 1.025478708 sum 1.36116937
# Socket and IP used for each connection:
[0]  12 socket used, resolved to 127.0.0.1:30000, connection timing : count 12 avg 0.00037757642 +/- 0.0003096 min 0.000155458 max 0.001240125 sum 0.004530917
[1]  17 socket used, resolved to 127.0.0.1:30000, connection timing : count 17 avg 0.00031488971 +/- 7.994e-05 min 0.000194917 max 0.000489667 sum 0.005353125
Sockets used: 29 (for perfect keepalive, would be 2)
Uniform: false, Jitter: true, Catchup allowed: true
IP addresses distribution:
127.0.0.1:30000: 29
Code 200 : 32 (55.2 %)
Code 500 : 26 (44.8 %)
All done 58 calls (plus 2 warmup) 1015.301 ms avg, 1.8 qps
```


로그를 확인해보면, 503(UO = Upstream Overflow)를 확인할 수 있다.


```bash
kubectl logs -n istioinaction -l app=simple-web -c istio-proxy -f
...
[2025-05-01T07:15:51.354Z] "GET // HTTP/1.1" 503 UO upstream_reset_before_response_started{overflow} - "-" 0 81 0 - "172.18.0.1" "fortio.org/fortio-1.69.4" "f2cc54d7-9bd1-9862-8198-b8a8b7dd685b" "simple-backend:80" "10.10.0.50:8080" outbound|80||simple-backend.istioinaction.svc.cluster.local - 10.200.1.221:80 172.18.0.1:0 - -
[2025-05-01T07:15:51.351Z] "GET / HTTP/1.1" 500 - via_upstream - "-" 0 687 4 4 "172.18.0.1" "fortio.org/fortio-1.69.4" "f2cc54d7-9bd1-9862-8198-b8a8b7dd685b" "simple-web.istioinaction.io:30000" "10.10.0.51:8080" inbound|8080|| 127.0.0.6:40413 10.10.0.51:8080 172.18.0.1:0 outbound_.80_._.simple-web.istioinaction.svc.cluster.local default
```


HTTP에 대한 동시 요청개수를 늘려보자, `http2MaxRequests (1 → 2)`


```bash
kubectl patch destinationrule simple-backend-dr -n istioinaction \
-n istioinaction --type merge --patch \
'{"spec": {"trafficPolicy": {"connectionPool": {"http": {"http2MaxRequests": 2}}}}}'

destinationrule.networking.istio.io/simple-backend-dr patched
```


대부분의 요청이 성공하는 것을 볼 수 있다. `Code 200 : 31 (83.8 %)`


```bash
fortio load -quiet -jitter -t 30s -c 2 -qps 2 --allow-initial-errors http://simple-web.istioinaction.io:30000
...
Sockets used: 8 (for perfect keepalive, would be 2)
Uniform: false, Jitter: true, Catchup allowed: true
IP addresses distribution:
127.0.0.1:30000: 8
Code 200 : 31 (83.8 %)
Code 500 : 6 (16.2 %)
All done 37 calls (plus 2 warmup) 1613.922 ms avg, 1.2 qps
```


이때 status를 확인해보면 아래와 같이 `cx_overflow`에 비해 `rq_pending_overflow`값이 낮아진 것을 볼 수 있다. 실패한 요청은 `rq_pending` (http1MaxPendingRequests)에 걸려 서킷 브레이커가 작동된 경우로 보인다.


```bash
kubectl exec -it deploy/simple-web -c istio-proxy -n istioinaction \
 -- curl localhost:15000/stats | grep simple-backend | grep overflow
cluster.outbound|80||simple-backend.istioinaction.svc.cluster.local.upstream_cx_overflow: 36
cluster.outbound|80||simple-backend.istioinaction.svc.cluster.local.upstream_cx_pool_overflow: 0
cluster.outbound|80||simple-backend.istioinaction.svc.cluster.local.upstream_rq_pending_overflow: 6
cluster.outbound|80||simple-backend.istioinaction.svc.cluster.local.upstream_rq_retry_overflow: 0
```


마지막으로 rq_pending(http1MaxPendingRequests)을 여유롭게 늘려보자. (1→2)


```bash
kubectl patch destinationrule simple-backend-dr \
-n istioinaction --type merge --patch \
'{"spec": {"trafficPolicy": {"connectionPool": {"http": {"http1MaxPendingRequests": 2}}}}}'

destinationrule.networking.istio.io/simple-backend-dr patched
```


한번 더 istio proxy의 통계를 초기화해주고, 테스트를 진행한다. 


아래와 같이 모두 성공하는 것을 볼 수 있다. `Code 200 : 34 (100.0 %)`


```bash
fortio load -quiet -jitter -t 30s -c 2 -qps 2 --allow-initial-errors http://simple-web.istioinaction.io:30000
...
[0]   1 socket used, resolved to 127.0.0.1:30000, connection timing : count 1 avg 0.001037542 +/- 0 min 0.001037542 max 0.001037542 sum 0.001037542
[1]   1 socket used, resolved to 127.0.0.1:30000, connection timing : count 1 avg 0.000206667 +/- 0 min 0.000206667 max 0.000206667 sum 0.000206667
Sockets used: 2 (for perfect keepalive, would be 2)
Uniform: false, Jitter: true, Catchup allowed: true
IP addresses distribution:
127.0.0.1:30000: 2
Code 200 : 34 (100.0 %)
All done 34 calls (plus 2 warmup) 1831.068 ms avg, 1.1 qps
```


istio proxy의 통계값을 봐도, upstream_cx_overflow만 발생한 것을 볼 수 있다.


```bash
kubectl exec -it deploy/simple-web -c istio-proxy -n istioinaction \
 -- curl localhost:15000/stats | grep simple-backend | grep overflow
cluster.outbound|80||simple-backend.istioinaction.svc.cluster.local.upstream_cx_overflow: 48
cluster.outbound|80||simple-backend.istioinaction.svc.cluster.local.upstream_cx_pool_overflow: 0
cluster.outbound|80||simple-backend.istioinaction.svc.cluster.local.upstream_rq_pending_overflow: 0
cluster.outbound|80||simple-backend.istioinaction.svc.cluster.local.upstream_rq_retry_overflow: 0
```


#### 이상값 탐지


특정 엔드포인트에 이상이 확인되었을 때, 이를 자동으로 감지하여 해당 엔드포인트를 제거하는 istio의 기능을 확인해본다.


우선 75%의 확률로 장애가 발생하는 파드를 배포한다.


```bash
kubectl apply -n istioinaction -f ch6/simple-backend-periodic-failure-500.yaml
```


설정값 주입


```bash
kubectl exec -it deploy/simple-backend-1 -n istioinaction -- sh

/ # export ERROR_TYPE=http_error
/ # export ERROR_RATE=0.75
/ # export ERROR_CODE=500
/ # exit
```


아래와 같이 `simple-backend-1-6b5d96b75-szthx`는 비정상적인 파드고, 나머지 2개의 파드는 정상파드이다.


```bash
kubectl get deploy,pod -n istioinaction -o wide

NAME                               READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS       IMAGES                                 SELECTOR
deployment.apps/simple-backend-1   1/1     1            1           4d8h   simple-backend   nicholasjackson/fake-service:v0.14.1   app=simple-backend
deployment.apps/simple-backend-2   2/2     2            2           4d8h   simple-backend   nicholasjackson/fake-service:v0.17.0   app=simple-backend
deployment.apps/simple-web         1/1     1            1           4d8h   simple-web       nicholasjackson/fake-service:v0.17.0   app=simple-web

NAME                                    READY   STATUS    RESTARTS   AGE     IP           NODE                  NOMINATED NODE   READINESS GATES
pod/simple-backend-1-6b5d96b75-szthx    2/2     Running   0          45s     10.10.0.55   myk8s-control-plane   <none>           <none>
pod/simple-backend-2-6876494bbf-brvlz   2/2     Running   0          5m44s   10.10.0.52   myk8s-control-plane   <none>           <none>
pod/simple-backend-2-6876494bbf-c6h84   2/2     Running   0          5m44s   10.10.0.54   myk8s-control-plane   <none>           <none>
pod/simple-web-6d865cd696-n5wbk         2/2     Running   0          97m     10.10.0.51   myk8s-control-plane   <none>           <none>
```


이제 DestinatinoRule 설정을 통해 이상감지(`outlierDetection`) 설정을 진행한다. 


```yaml
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: simple-backend-dr
spec:
  host: simple-backend.istioinaction.svc.cluster.local
  trafficPolicy:
    outlierDetection:
      consecutive5xxErrors: 1 # 잘못된 요청이 하나만 발생해도 이상값 감지가 발동. 기본값 5
      interval: 5s # 이스티오 서비스 프록시가 체크하는 주기. 기본값 10초. Time interval between ejection sweep analysis
      baseEjectionTime: 5s # 서비스 엔드포인트에서 제거된다면, 제거 시간은 n(해당 엔드포인트가 쫓겨난 횟수) * baseEjectionTime. 해당 시간이 지나면 로드 밸런싱 풀에 다시 추가됨. 기본값 30초. 
      maxEjectionPercent: 100 # 로드 밸런싱 풀에서 제거 가능한 호스트 개수(%). 모든 호스트가 오동작하면 어떤 요청도 통과 못함(회로가 열린 것과 같다). 기본값 10%

```


현재 엔드포인트를 확인해보면 아래와 같다. 부하 테스트가 끝나고 엔드포인트의 설정값과 비교해보자.


```bash
docker exec -it myk8s-control-plane istioctl proxy-config endpoint deploy/simple-web.istioinaction --cluster 'outbound|80||simple-backend.istioinaction.svc.cluster.local'

ENDPOINT            STATUS      OUTLIER CHECK     CLUSTER
10.10.0.52:8080     HEALTHY     OK                outbound|80||simple-backend.istioinaction.svc.cluster.local
10.10.0.54:8080     HEALTHY     OK                outbound|80||simple-backend.istioinaction.svc.cluster.local
10.10.0.55:8080     HEALTHY     OK                outbound|80||simple-backend.istioinaction.svc.cluster.local
```


##### 부하테스트 진행


파드 3개중 1개에 이상 파드이며 75%의 확률로 문제가 발생한다. 계산해보면, 1/3 * 3/4 → 25%의 확률로 장애가 발생한다. 하지만, 테스트 결과 실패율은 그보다 낮다. `Code 500 : 5 (8.3 %)` 


이 결과로 이상감지 옵션이 정상적으로 작동했다는 것을 알 수 있다.


```bash
fortio load -quiet -jitter -t 30s -c 2 -qps 2 --allow-initial-errors http://simple-web.istioinaction.io:30000

Fortio 1.69.4 running at 2 queries per second, 10->10 procs, for 30s: http://simple-web.istioinaction.io:30000
Aggregated Function Time : count 60 avg 0.16303673 +/- 0.04507 min 0.011594209 max 0.195608333 sum 9.78220388
...
IP addresses distribution:
127.0.0.1:30000: 7
Code 200 : 55 (91.7 %)
Code 500 : 5 (8.3 %)
All done 60 calls (plus 2 warmup) 163.037 ms avg, 2.0 qps
```


Istio proxy config로 자세하게 확인해보면, 아래와 같이 문제가 있는 파드 상태를 `FAILED`로 업데이트한 것을 확인할 수 있다.


```bash
ENDPOINT            STATUS      OUTLIER CHECK     CLUSTER
10.10.0.52:8080     HEALTHY     OK                outbound|80||simple-backend.istioinaction.svc.cluster.local
10.10.0.54:8080     HEALTHY     OK                outbound|80||simple-backend.istioinaction.svc.cluster.local
10.10.0.55:8080     HEALTHY     FAILED            outbound|80||simple-backend.istioinaction.svc.cluster.local
```


istio proxy의 통계값을 보면 아래와 같이 잘못된 파드에 대한 엔드포인트를 추출(_**`ejections_total`**_) 한 것을 볼 수 있다.


```bash
kubectl exec -it deploy/simple-web -c istio-proxy -n istioinaction \
 -- curl localhost:15000/stats | grep simple-backend | grep outlier
 ...
cluster.outbound|80||simple-backend.istioinaction.svc.cluster.local.outlier_detection.ejections_total: 3
```


### 마치며


이번 편에서는 istio에서 제공하는 트래픽 제어 방식(통제, shifting, mirroring, 외부 서비스와 연결 및 차단)에 대해 살펴봤다. 다음 편에서는 istio의 네트워크 복원성에 대해 살펴볼 예정이다.

