---
layout: post
title: EKS ìŠ¤í„°ë”” 5ì£¼ì°¨
date: 2023-11-11 13:54 +0900
description: CloudNetì—ì„œ ì£¼ê´€í•˜ëŠ” EKS ìŠ¤í„°ë”” 5ì£¼ì°¨ë¥¼ ì§„í–‰í•˜ë©° ë°°ìš´ ë‚´ìš©ì„ ì •ë¦¬í•œë‹¤.
image:
  path: /assets/img/post/2023-11-11-eks-ìŠ¤í„°ë””-ìë£Œ/5ì£¼ì°¨.jpg
  alt: EKS ìŠ¤í„°ë”” 5ì£¼ì°¨
category: [Study, EKS]
tags: [EKS, CloudNet, VPC]
pin: false
math: true
mermaid: true
---
CloudNetì—ì„œ ì£¼ê´€í•˜ëŠ” EKS ìŠ¤í„°ë”” 5ì£¼ì°¨ë¥¼ ì§„í–‰í•˜ë©° ë°°ìš´ ë‚´ìš©ì„ ì •ë¦¬í•œë‹¤.
<!--more-->
## ìš”ì•½

ì´ë²ˆ ì£¼ì°¨ì—ëŠ” ì˜¤í† ìŠ¤ì¼€ì¼ë§ì— ëŒ€í•´ ì§„í–‰í–ˆë‹¤. íŒŒë“œì˜ ì˜¤í† ìŠ¤ì¼€ì¼ë§ìœ¼ë¡œ HPA, VPA ê·¸ë¦¬ê³  ë…¸ë“œì— ë¹„ë¡€í•´ì„œ íŒŒë“œ ê°œìˆ˜ë¥¼ ì¡°ì •í•˜ëŠ” CPAê¹Œì§€ ìˆë‹¤. ë…¸ë“œë¥¼ ìŠ¤ì¼€ì¤„ë§í•˜ëŠ” KEDA, CA, Karpenter ê¹Œì§€ ì§„í–‰í•˜ë©´ì„œ ì´ë²ˆì£¼ì°¨ëŠ” ëë‚œë‹¤.

íŒŒë“œì˜ ìŠ¤ì¼€ì¤„ë§ì€ íŒŒë“œì˜ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ì„ ê¸°ì¤€ìœ¼ë¡œ ì§„í–‰í•œë‹¤. íŒŒë“œì˜ ê°œìˆ˜ë¥¼ ëŠ˜ë¦¬ëŠ” ê²ƒì€ ë¶€í•˜ë¶„ì‚°ì´ ì—†ë‹¤ë©´ ì˜ë¯¸ì—†ëŠ” ì¼ì´ì§€ë§Œ, `selector` ë¥¼ í†µí•œ ë¶€í•˜ë¶„ì‚°ì„ ì§€ì›í•˜ê¸°ì— íš¨ê³¼ì ì´ë‹¤. ë˜í•œ, VPAëŠ” ë¦¬ì†ŒìŠ¤ë¥¼ ëŠ˜ë¦¬ë‚˜ ì¬ì‹¤í–‰ì´ í•„ìš”í•˜ë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤. [ì•ìœ¼ë¡œ ì¬ì‹¤í–‰ì—†ì´ ë¦¬ì†ŒìŠ¤ ë³€ê²½ì´ ê°€ëŠ¥í• ì§€ë„ ëª¨ë¥¸ë‹¤. ([Docs](https://kubernetes.io/blog/2023/05/12/in-place-pod-resize-alpha/))], HPA -VPA ëª¨ë‘ ì¿ ë²„ë„¤í‹°ìŠ¤ì—ì„œ ì§€ì›í•´ì¤˜, í”ŒëŸ¬ê·¸ì¸ ì„¤ì¹˜ ì—†ì´ ì§„í–‰ê°€ëŠ¥í•˜ë‹¤. 

ë…¸ë“œë¥¼ ìŠ¤ì¼€ì¤„ë§í•˜ëŠ” CA, KEDA, Karpenterê°€ ìˆë‹¤. CAëŠ” íŒŒë“œë¥¼ í†µí•´ ëª¨ë‹ˆí„°ë§í•˜ê³ , ë¦¬ì†ŒìŠ¤ ë©”íŠ¸ë¦­ì„ í†µí•´ ìŠ¤ì¼€ì¼ë§í•œë‹¤. KEDAëŠ” CAì™€ëŠ” ë‹¤ë¥´ê²Œ ë¦¬ì†ŒìŠ¤ ë©”íŠ¸ë¦­ì´ ì•„ë‹Œ ì´ë²¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ìŠ¤ì¼€ì¼ì—¬ë¶€ë¥¼ ê²°ì •í•œë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ KarpenterëŠ” ë‹¤ë¥¸ ì˜¤í† ìŠ¤ì¼€ì¼ë§ê³¼ ë‹¤ë¥´ê²Œ ì´ˆ ë‹¨ìœ„ë¡œ ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ë¥¼ ì œê³µí•œë‹¤. 

Karpenterë¥¼ ë§ˆì§€ë§‰ìœ¼ë¡œ ì‹¤ìŠµì€ ì¢…ë£Œëœë‹¤. ì£¼ì œê°€ ì˜¤í† ìŠ¤ì¼€ì¼ë§ì´ë‹¤ë³´ë‹ˆ ëë‚˜ê³  ë‚˜ì„œ ìì›ì„ ê¼­ ì‚­ì œí•´ì¤˜ì•¼í•œë‹¤. 

## ìš©ì–´ì„¤ëª…

- ìŠ¤ì¼€ì¼ ì•„ì›ƒ, ìŠ¤ì¼€ì¼ ì¸

ìŠ¤ì¼€ì¼ ì•„ì›ƒì€ ê¸°ì¡´ì˜ ì¸í”„ë¼ì´ì™¸ì— ìƒˆë¡œìš´ ì¸í”„ë¼ë¥¼ ì¶”ê°€í•´ì„œ í™•ì¥í•˜ëŠ” ë°©ì‹ì´ë‹¤. â€˜ìŠ¤ì¼€ì¼ì¸â€™ì€ ë°˜ëŒ€

- ìŠ¤ì¼€ì¼ ì—…, ìŠ¤ì¼€ì¼ ë‹¤ìš´ 

ìŠ¤ì¼€ì¼ ì—…ì€ ê¸°ì¡´ì˜ ì¸í”„ë¼ë¥¼ í™•ì¥í•˜ëŠ” ê²ƒì´ë‹¤. â€˜ìŠ¤ì¼€ì¼ ë‹¤ìš´â€™ì€ ë°˜ëŒ€! ex) CPU ë³€ê²½, RAMì¶”ê°€

## ë°°í¬í™˜ê²½

ê¸°ì¡´ì˜ ë°°í¬í™˜ê²½ê³¼ ë™ì¼í•˜ê²Œ, kube-ops-view, í”„ë¡œë©”í…Œìš°ìŠ¤, ê·¸ë¼íŒŒë‚˜ê¹Œì§€ ì„¤ì¹˜í•œë‹¤. ê´€ë ¨ëœ ë°°í¬ëŠ” ì´ì „ì£¼ì°¨ë¥¼ ì°¸ê³ í•˜ë©´ ëœë‹¤.

ì´ë²ˆ ì£¼ì°¨ì—ì„œëŠ” ë…¸ë“œì˜ ì˜¤í† ìŠ¤ì¼€ì¼ë§ì„ í™•ì¸í•˜ê¸° ìœ„í•´ EKS Node Viewerë¥¼ ì„¤ì¹˜í•˜ì—¬ ê° ë…¸ë“œë“¤ì˜ CPU ì‚¬ìš©ëŸ‰ì„ ëª¨ë‹ˆí„°ë§í•œë‹¤.

> **EKS Node Viewer** ì„¤ì¹˜ : **ë…¸ë“œ í• ë‹¹ ê°€ëŠ¥ ìš©ëŸ‰**ê³¼ **ìš”ì²­ request ë¦¬ì†ŒìŠ¤ í‘œì‹œ**, **ì‹¤ì œ íŒŒë“œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ X**
> 

```bash
# Go ì„¤ì¹˜
$yum install -y go
Loaded plugins: extras_suggestions, langpacks, priorities, update-motd
...
# EKS Node Viewer ì„¤ì¹˜
$go install github.com/awslabs/eks-node-viewer/cmd/eks-node-viewer@latest

$tree ~/go/bin
/root/go/bin
â””â”€â”€ eks-node-viewer

```

ì„¤ì¹˜ë¥¼ ë§ˆë¬´ë¦¬ í•˜ê³ , ë””ë ‰í† ë¦¬ì— ë“¤ì–´ê°€ `./eks-node-viewer` ë¥¼ ì‹¤í–‰í•˜ë©´ ì•„ë˜ì™€ ê°™ì´ ë…¸ë“œë¥¼ ëª¨ë‹ˆí„°ë§í™”ë©´ì´ ë‚˜ì˜¨ë‹¤.

![](https://velog.velcdn.com/images/han-0315/post/24711f0a-a5a1-498f-80ae-91064be028d9/image.png)


## Pod AutuScaling

í•´ë‹¹ íŒŒíŠ¸ì—ì„œëŠ” íŒŒë“œì˜ ì˜¤í† ìŠ¤ì¼€ì¼ë§ì— ëŒ€í•´ ì‹¤ìŠµí•œë‹¤. ì•„ë˜ì˜ ê·¸ë¦¼ì—ì„œ ì•Œ ìˆ˜ ìˆë“¯ì´ ê°™ì€ ìŠ¤í™ì˜ íŒŒë“œë¥¼ ì¦ê°€ì‹œí‚¤ëŠ” HPAì™€ íŒŒë“œì˜ ë¦¬ì†ŒìŠ¤ë¥¼ ì¦ê°€ì‹œí‚¤ëŠ” VPAê°€ ìˆë‹¤.

![](https://velog.velcdn.com/images/han-0315/post/c819aee0-9736-4f54-802b-55f281cea929/image.png)


### HPA - Horizontal Pod Autoscaler

HPAëŠ” ë¦¬ì†ŒìŠ¤ ë©”íŠ¸ë¦­ì„ í†µí•´ íŒŒë“œì˜ ë¦¬ì†ŒìŠ¤ë¥¼ íŒŒì•…í•˜ì—¬ ìŠ¤ì¼€ì¼ë§í•œë‹¤. ì•„ë˜ì˜ ì•„í‚¤í…ì²˜ë¥¼ í†µí•´ì„œ ìì„¸í•œ ì›ë¦¬ë¥¼ ì•Œ ìˆ˜ ìˆë‹¤. HPAë¡œ ì§„í–‰ë˜ëŠ” íŒŒë“œëŠ” ë¡œë“œë°¸ëŸ°ì‹± ì„¤ì •ì„ ì§„í–‰í•´ì•¼ í•œë‹¤. ë¡œë“œë°¸ëŸ°ì‹±ì„ í†µí•´ í•˜ë‚˜ì˜ íŒŒë“œë§Œ ë¶€í•˜ë¥¼ ë°›ëŠ” ê²ƒì´ ì•„ë‹Œ ìƒˆë¡œìš´ íŒŒë“œê¹Œì§€ ë¶€í•˜ë¥¼ ë¶„ë‹´í•œë‹¤.(selector) ê·¸ë ‡ê¸°ì— íš¨ìœ¨ì ìœ¼ë¡œ ë¦¬ì†ŒìŠ¤ë¥¼ ê´€ë¦¬í•  ìˆ˜ ìˆë‹¤. 


![](https://velog.velcdn.com/images/han-0315/post/644c35db-8ef8-4da3-b957-da3f1d9bb4be/image.png)


**ê·¸ë¦¼ ì¶œì²˜ - (ğŸ§ğŸ»â€â™‚ï¸)ê¹€íƒœë¯¼ ê¸°ìˆ  ë¸”ë¡œê·¸ - [ë§í¬](https://kubetm.github.io/k8s/08-intermediate-controller/hpa/)**

í…ŒìŠ¤íŠ¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ë°°í¬

```bash
$curl -s -O https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/php-apache.yaml
$cat php-apache.yaml | yh
apiVersion: apps/v1
kind: Deployment
metadata:
  name: php-apache
spec:
  selector:
    matchLabels:
      run: php-apache
  template:
    metadata:
      labels:
        run: php-apache
    spec:
      containers:
      - name: php-apache
        image: registry.k8s.io/hpa-example
        ports:
        - containerPort: 80
        resources:
          limits:
            cpu: 500m
          **requests:
            cpu: 200m**
---
apiVersion: v1
kind: Service
metadata:
  name: php-apache
  labels:
    run: php-apache
spec:
  ports:
  - port: 80
  selector:
    run: php-apache

# í…ŒìŠ¤íŠ¸ ìš© php-apache server ë°°í¬
$k apply -f php-apache.yaml
deployment.apps/php-apache created
service/php-apache created

# í™•ì¸, ì—°ì‚°ì˜ ë³µì¡ë„ë¥¼ ì£¼ì—ˆë‹¤.
$kubectl exec -it deploy/php-apache -- cat /var/www/html/index.php
<?php
$x = 0.0001;
for ($i = 0; $i <= 1000000; $i++) {
	$x += sqrt($x);
}
echo "OK!";
?>

$PODIP=$(kubectl get pod -l run=php-apache -o jsonpath={.items[0].status.podIP})
# ìœ„ì™€ ê°™ì´ ì»¤ë¦¬ë¥¼ ë‚ ë¦¬ë©´ ìœ„ì˜ ì—°ì‚° í›„ íŠ¸ë˜í”½ì´ ë‚ ì•„ì˜¨ë‹¤.
$curl -s $PODIP; echo
OK!
```

HPA ì„¤ì • ë° í™•ì¸

```bash
# cpu ì‚¬ìš©ëŸ‰ì„ ê¸°ì¤€ìœ¼ë¡œ HPA
$kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10
horizontalpodautoscaler.autoscaling/php-apache autoscaled
# í™•ì¸ ê°€ëŠ¥
$kubectl describe hpa
Warning: autoscaling/v2beta2 HorizontalPodAutoscaler is deprecated in v1.23+, unavailable in v1.26+; use autoscaling/v2 HorizontalPodAutoscaler
Name:                                                  php-apache
Namespace:                                             default
Labels:                                                <none>
Annotations:                                           <none>
CreationTimestamp:                                     Sun, 21 May 2023 22:33:48 +0900
Reference:                                             Deployment/php-apache
Metrics:                                               ( current / target )
  resource cpu on pods  (as a percentage of request):  0% (1m) / 50%
Min replicas:                                          1
Max replicas:                                          10
Deployment pods:                                       1 current / 1 desired
Conditions:
  Type            Status  Reason               Message
  ----            ------  ------               -------
  AbleToScale     True    ScaleDownStabilized  recent recommendations were higher than current one, applying the highest recent recommendation
  ScalingActive   True    ValidMetricFound     the HPA was able to successfully calculate a replica count from cpu resource utilization (percentage of request)
  ScalingLimited  False   DesiredWithinRange   the desired count is within the acceptable range
Events:           <none>

```

ë§¤ë‹ˆí˜ìŠ¤íŠ¸ì—ì„œ ìƒíƒœê°’ì€ ë²„ë¦¬ê³ , ì •ë¦¬í•´ì£¼ëŠ” íˆ´ì¸ `neat` í”ŒëŸ¬ê·¸ì¸ì„ ì„¤ì¹˜í•œë‹¤.

```bash
$kubectl krew install neat
...
# ìƒíƒœê°’ê¹Œì§€ ì¶œë ¥ë˜ì–´ ë³´ê¸° ë¶ˆí¸í•˜ë‹¤.
$kubectl get hpa php-apache -o yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  creationTimestamp: "2023-05-21T13:33:48Z"
  name: php-apache
  namespace: default
  resourceVersion: "6344"
  uid: cae373a4-fb84-4ff3-9945-d19984fe811c
spec:
  maxReplicas: 10
  metrics:
  - resource:
      name: cpu
      target:
        averageUtilization: 50
        type: Utilization
    type: Resource
  minReplicas: 1
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: php-apache
status:
  conditions:
  - lastTransitionTime: "2023-05-21T13:34:03Z"
    message: recent recommendations were higher than current one, applying the highest
      recent recommendation
    reason: ScaleDownStabilized
    status: "True"
    type: AbleToScale
  - lastTransitionTime: "2023-05-21T13:34:03Z"
    message: the HPA was able to successfully calculate a replica count from cpu resource
      utilization (percentage of request)
    reason: ValidMetricFound
    status: "True"
    type: ScalingActive
  - lastTransitionTime: "2023-05-21T13:34:03Z"
    message: the desired count is within the acceptable range
    reason: DesiredWithinRange
    status: "False"
    type: ScalingLimited
  currentMetrics:
  - resource:
      current:
        averageUtilization: 0
        averageValue: 1m
      name: cpu
    type: Resource
  currentReplicas: 1
  desiredReplicas: 1

# ê¹”ë”í•˜ê²Œ ì¶œë ¥ë˜ëŠ” ëª¨ìŠµ
# ì•„ë˜ì˜ ë‚´ìš©ì„ í™•ì¸í•˜ë©´ ìµœëŒ€ 10ê°œì´ê³  ê¸°ì¤€ì€ CPUì˜ í‰ê· í™œìš©ëŸ‰ 50$ì´ë‹¤!, 50%ë¥¼ ë„˜ìœ¼ë©´ ìë™ìœ¼ë¡œ ìƒˆë¡œìš´ íŒŒë“œê°€ ìƒì„±ëœë‹¤.
$kubectl get hpa php-apache -o yaml | kubectl neat | yh
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: php-apache
  namespace: default
spec:
  **maxReplicas: 10**
  metrics:
  - resource:
      **name: cpu**
      target:
        **averageUtilization: 50**
        type: Utilization
    type: Resource
  minReplicas: 1
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: php-apache
```

ë¶€í•˜ë¥¼ ì£¼ì–´ í…ŒìŠ¤íŠ¸í•˜ëŠ” ëª¨ìŠµ, íŒŒë“œê°€ ì¦ê°€í•˜ëŠ” ëª¨ìŠµì€ ì•„ë˜ì˜ ì‚¬ì§„ì„ í†µí•´ ìì„¸í•˜ê²Œ í™•ì¸í•  ìˆ˜ ìˆë‹¤!

```bash

$while true;do curl -s $PODIP; sleep 0.5; done
OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!^C
$while true;do curl -s $PODIP; sleep 0.3; done
OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!^C
$kubectl run -i --tty load-generator --rm --image=busybox:1.28 --restart=Never -- /bin/sh -c "while sleep 0.01; do wget -q -O- http://php-apache; done"
If you don't see a command prompt, try pressing enter.
OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!
OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!
OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!

$kubectl delete deploy,svc,hpa,pod --all
deployment.apps "php-apache" deleted
service "kubernetes" deleted
service "php-apache" deleted
horizontalpodautoscaler.autoscaling "php-apache" deleted
pod "load-generator" deleted
pod "php-apache-698db99f59-4s7d7" deleted
pod "php-apache-698db99f59-qkd2h" deleted
pod "php-apache-698db99f59-wcxvg" deleted
pod "php-apache-698db99f59-wh62k" deleted
pod "php-apache-698db99f59-x2x5r" deleted
pod "php-apache-698db99f59-zblfs" deleted
```

íŒŒë“œ í•˜ë‚˜ë‹¹ CPUí™œìš©ëŸ‰ì´ 50% ë¯¸ë§Œì´ë˜ë ¤ë©´ 6~7ê°œì •ë„ì˜ íŒŒë“œê°€ ìƒì„±ë˜ì–´ì•¼ í–ˆë‹¤. apache ì„œë¹„ìŠ¤ë¥¼ í†µí•´ ì§€ì†ì ìœ¼ë¡œ íŠ¸ë˜í”½ì„ ë‚ ë¦¬ëŠ” ê²ƒì´ë¯€ë¡œ `selector` ì— ì˜í•´ ìë™ìœ¼ë¡œ ë¡œë“œë°¸ëŸ°ì‹±ì´ ëœë‹¤. ê·¸ë ‡ê¸°ì— ë¶€í•˜ê°€ ë¶„ì‚°ì´ ê°€ëŠ¥í•´ 7ê°œë©´ 50%ë¯¸ë§Œìœ¼ë¡œ ë–¨ì–´ì§„ë‹¤.


![](https://velog.velcdn.com/images/han-0315/post/d0714bc2-0101-40ec-b760-a2fee9cbc5e2/image.png)


**ê·¸ë¼íŒŒë‚˜ ëŒ€ì‹œë³´ë“œë¥¼ í†µí•´ í™•ì¸í•œ íŒŒë“œì˜ ê°œìˆ˜!**

![](https://velog.velcdn.com/images/han-0315/post/e824835f-9fbf-47b3-98f1-3caf169a244e/image.png)


### **VPA - Vertical Pod Autoscaler**

HPAëŠ” ë™ì¼í•œ ìŠ¤í™ì˜ íŒŒë“œì˜ ê°œìˆ˜ë¥¼ ëŠ˜ë¦¬ëŠ” ê²ƒì´ì—ˆë‹¤ë©´, VPAëŠ” í˜„ì¬ ì‹¤í–‰ì¤‘ì¸ íŒŒë“œì˜ ë¦¬ì†ŒìŠ¤ë¥¼ í™•ì¥í•œë‹¤. ì´ ê³¼ì •ì—ì„œ ì¬ì‹œì‘ì´ í•„ìš”í•˜ë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤. íŒŒë“œë¥¼ ì¬ì‹œì‘í•´ì•¼ë˜ê¸°ì—, ì‹¤ì œ ìš´ì˜í™˜ê²½ì—ì„œëŠ” ë³„ë¡œ ì¶”ì²œí•˜ì§€ ì•ŠëŠ”ë‹¤ê³  í•œë‹¤. ìŠ¤í„°ë””ì›ë¶„ì´ ì•Œë ¤ì£¼ì‹  ì•„ë˜ì˜ ë¬¸ì„œì™€ ê°™ì´ ì¬ì‹¤í–‰ì—†ì´ ë¦¬ì†ŒìŠ¤ ë³€ê²½ì´ ê°€ëŠ¥í•˜ë‹¤ë©´ ìš´ì˜í™˜ê²½ì—ì„œë„ ì“¸ëª¨ìˆì„ ê²ƒ ê°™ë‹¤.

> [https://kubernetes.io/blog/2023/05/12/in-place-pod-resize-alpha/](https://kubernetes.io/blog/2023/05/12/in-place-pod-resize-alpha/)
> 

ë˜í•œ, VPAëŠ” ë©”íŠ¸ë¦­ì„œë²„ë¥¼ í†µí•´ íŒŒë“œì˜ ë¡œê·¸ë¥¼ ìˆ˜ì§‘í•˜ì—¬ ë¦¬ì†ŒìŠ¤ ìµœì ê°’ì„ ì¶”ì²œí•´ì¤€ë‹¤. ì•„ë˜ì˜ ì•„í‚¤í…ì²˜ë¥¼ í†µí•´ ì‘ë™ë°©ì‹ì„ íŒŒì•…í•  ìˆ˜ ìˆë‹¤. 

[ì•…ë¶„ë‹˜ì˜ ë¸”ë¡œê·¸ ì°¸ê³ ! [https://malwareanalysis.tistory.com/603](https://malwareanalysis.tistory.com/603)]

![](https://velog.velcdn.com/images/han-0315/post/1c84e6f1-6249-49e2-9114-eacecfe6b8cc/image.png)

ê·¸ë¦¼ ì¶œì²˜ : [Blog](https://devocean.sk.com/blog/techBoardDetail.do?ID=164786&boardType=techBlog&searchData=&page=&subIndex=%EC%B5%9C%EC%8B%A0+%EA%B8%B0%EC%88%A0+%EB%B8%94%EB%A1%9C%EA%B7%B8)

ì´ì œ ì¿ ë²„ë„¤í‹°ìŠ¤ì—ì„œ ì œê³µí•œ ì˜ˆì‹œë¥¼ í†µí•´ ì‹¤ìŠµì„ ì§„í–‰í•œë‹¤.

```bash
# ì¿ ë²„ë„¤í‹°ìŠ¤ ì˜ˆì‹œ í”„ë¡œì íŠ¸ 
git clone https://github.com/kubernetes/autoscaler.git
cd ~/autoscaler/vertical-pod-autoscaler/
# openssl ì—…ê·¸ë ˆì´ë“œ, (v1.1.1 ì´ìƒì´ì–´ì•¼ í•œë‹¤.)
yum install openssl11 -y
sed -i 's/openssl/openssl11/g' ~/autoscaler/vertical-pod-autoscaler/pkg/admission-controller/gencerts.sh
# ë°°í¬!
kubectl apply -f examples/hamster.yaml && kubectl get vpa -w

```

ì•„ë˜ëŠ” ë°°í¬ëœ `yaml` íŒŒì¼ì´ë‹¤. íŒŒì¼ì—ì„œ ì„¤ì •ëœ ë¦¬ì†ŒìŠ¤ cpu 100m, memory 50Mië¥¼ í™•ì¸í•  ìˆ˜ ìˆì§€ë§Œ, ì¶”í›„ VPAì— ì˜í•´ íŒŒë“œê°€ ì¬ìƒì„±ë˜ë©´ì„œ ë¦¬ì†ŒìŠ¤ë¥¼ ë³€ê²½ì‹œí‚¨ë‹¤. 

```bash
# ì•„ë˜ì˜ ì˜ˆì‹œ yaml íŒŒì¼ì—ì„œ ì„¤ì •ëœ ë¦¬ì†ŒìŠ¤ cpu 100m, memory 50Mi
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hamster
spec:
  selector:
    matchLabels:
      app: hamster
  replicas: 2
  template:
    metadata:
      labels:
        app: hamster
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 65534 # nobody
      containers:
        - name: hamster
          image: registry.k8s.io/ubuntu-slim:0.1
          resources:
            requests:
              cpu: 100m
              memory: 50Mi
          command: ["/bin/sh"]
          args:
            - "-c"
            - "while true; do timeout 0.5s yes >/dev/null; sleep 0.5s; done"
```

ì•„ë˜ì˜ í„°ë¯¸ë„ ì‚¬ì§„ì—ì„œ ìƒìœ„ë¥¼ ë³´ë©´ VPAì— ì˜í•´ íŒŒë“œê°€ ì¬ìƒì„±ëœ ê²ƒì„ ì•Œ ìˆ˜ ìˆê³ , í•˜ë‹¨ ì˜¤ë¥¸ìª½ì„ ë³´ë©´ CPU ìš”ì²­ëŸ‰ì´ ë‹¬ë¼ì§„ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

![](https://velog.velcdn.com/images/han-0315/post/09879010-1094-42d2-a62f-50f1fdb2eca6/image.png)


### **KRR**

ì´ë²ˆì— AWSì—ì„œ ì§€ì›í•´ì£¼ëŠ” íˆ´ì´ë©°, ë©”íŠ¸ë¦­ì„ ìˆ˜ì§‘í•˜ê³  ìŠ¤ìŠ¤ë¡œ íŒë‹¨í•˜ì—¬ ì¶”ì²œì„ í•´ì¤€ë‹¤ê³  í•œë‹¤. ì•„ë˜ì˜ ê·¸ë¦¼ì„ ì°¸ê³ í•˜ë©´ ìì„¸í•˜ê²Œ ë¦¬ì†ŒìŠ¤ë³„ ì¶”ì²œì„ í•´ì£¼ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

> `KRR` : Prometheus-based **K**ubernetes **R**esource **R**ecommendations - [ë§í¬](https://github.com/robusta-dev/krr#getting-started) & Youtube - [ë§í¬](https://www.youtube.com/live/uITOzpf82RY?feature=share)
> 

![](https://velog.velcdn.com/images/han-0315/post/2569d889-253b-4f7f-9ee2-b57225bb3a22/image.png)


## KEDA - Kubernetes based Event Driven Autoscaler

ë¦¬ì†ŒìŠ¤ ë©”íŠ¸ë¦­ì´ ì•„ë‹Œ íŠ¹ì • ì´ë²¤íŠ¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë…¸ë“œë¥¼ ì˜¤í† ìŠ¤ì¼€ì¼ë§í•œë‹¤.

KEDAëŠ” ì „ìš© ë©”íŠ¸ë¦­ì„œë²„ë¥¼ ë³„ë„ë¡œ ë‘”ë‹¤. ì•„ë§ˆ ë©”íŠ¸ë¦­ê´€ë ¨ ë‚´ìš©ì´ ì¡°ê¸ˆ ë‹¬ë¼ì„œ ê·¸ëŸ° ê²ƒ ê°™ë‹¤.!

ë˜í•œ, ê·¸ë¼íŒŒë‚˜ì˜ ê³µì‹ëŒ€ì‹œë³´ë“œì— ì—†ì–´ì„œ, Githubì—ì„œ configíŒŒì¼ì„ ë³µì‚¬í•˜ê³ , `import` í•œë‹¤. 

**ë°°í¬ëœ kube-ops-view ì‚¬ì§„**

```bash
# KEDA ì„¤ì¹˜
$cat <<EOT > keda-values.yaml
> metricsServer:
>   useHostNetwork: true
>
> prometheus:
>   metricServer:
>     enabled: true
>     port: 9022
>     portName: metrics
>     path: /metrics
>     serviceMonitor:
>       # Enables ServiceMonitor creation for the Prometheus Operator
>       enabled: true
>     podMonitor:
>       # Enables PodMonitor creation for the Prometheus Operator
>       enabled: true
>   operator:
>     enabled: true
>     port: 8080
>     serviceMonitor:
>       # Enables ServiceMonitor creation for the Prometheus Operator
>       enabled: true
>     podMonitor:
>       # Enables PodMonitor creation for the Prometheus Operator
>       enabled: true
>
>   webhooks:
>     enabled: true
>     port: 8080
>     serviceMonitor:
>       # Enables ServiceMonitor creation for the Prometheus webhooks
>       enabled: true
> EOT
# ë³„ë„ì˜ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„±
$kubectl create namespace keda
namespace/keda created

$helm repo add kedacore https://kedacore.github.io/charts
"kedacore" has been added to your repositories

# helmì„ í†µí•œ ë°°í¬
$helm install keda kedacore/keda --version 2.10.2 --namespace keda -f keda-values.yaml
NAME: keda
LAST DEPLOYED: Sun May 21 22:45:17 2023
NAMESPACE: keda
STATUS: deployed
REVISION: 1
TEST SUITE: None
# KEDA ì„¤ì¹˜ í™•ì¸
$kubectl get-all -n keda
W0521 22:45:24.843016   14118 client.go:102] Could not fetch complete list of API resources, results will be incomplete: unable to retrieve the complete list of server APIs: external.metrics.k8s.io/v1beta1: the server is currently unable to handle the request
NAME                                                                  NAMESPACE  AGE
configmap/kube-root-ca.crt                                            keda       26s
endpoints/keda-admission-webhooks                                     keda       5s
endpoints/keda-operator                                               keda       5s
endpoints/keda-operator-metrics-apiserver                             keda       5s
pod/keda-admission-webhooks-68cf687cbf-l6lpx                          keda       5s
pod/keda-operator-656478d687-4m47m                                    keda       5s
pod/keda-operator-metrics-apiserver-7fd585f657-xjltw                  keda       5s
secret/sh.helm.release.v1.keda.v1                                     keda       6s
serviceaccount/default                                                keda       26s
serviceaccount/keda-operator                                          keda       6s
service/keda-admission-webhooks                                       keda       5s
service/keda-operator                                                 keda       5s
service/keda-operator-metrics-apiserver                               keda       5s
deployment.apps/keda-admission-webhooks                               keda       5s
deployment.apps/keda-operator                                         keda       5s
deployment.apps/keda-operator-metrics-apiserver                       keda       5s
replicaset.apps/keda-admission-webhooks-68cf687cbf                    keda       5s
replicaset.apps/keda-operator-656478d687                              keda       5s
replicaset.apps/keda-operator-metrics-apiserver-7fd585f657            keda       5s
endpointslice.discovery.k8s.io/keda-admission-webhooks-jtdd2          keda       5s
endpointslice.discovery.k8s.io/keda-operator-7pdhn                    keda       5s
endpointslice.discovery.k8s.io/keda-operator-metrics-apiserver-9ltmf  keda       5s
podmonitor.monitoring.coreos.com/keda-operator                        keda       5s
podmonitor.monitoring.coreos.com/keda-operator-metrics-apiserver      keda       5s
servicemonitor.monitoring.coreos.com/keda-admission-webhooks          keda       5s
servicemonitor.monitoring.coreos.com/keda-operator                    keda       5s
servicemonitor.monitoring.coreos.com/keda-operator-metrics-apiserver  keda       5s
rolebinding.rbac.authorization.k8s.io/keda-operator                   keda       5s
role.rbac.authorization.k8s.io/keda-operator                          keda       6s
$kubectl get all -n keda
NAME                                                   READY   STATUS              RESTARTS   AGE
pod/keda-admission-webhooks-68cf687cbf-l6lpx           0/1     ContainerCreating   0          6s
pod/keda-operator-656478d687-4m47m                     0/1     ContainerCreating   0          6s
pod/keda-operator-metrics-apiserver-7fd585f657-xjltw   0/1     ContainerCreating   0          6s

NAME                                      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                   AGE
service/keda-admission-webhooks           ClusterIP   10.100.253.109   <none>        443/TCP,8080/TCP          6s
service/keda-operator                     ClusterIP   10.100.239.105   <none>        9666/TCP,8080/TCP         6s
service/keda-operator-metrics-apiserver   ClusterIP   10.100.137.4     <none>        443/TCP,80/TCP,9022/TCP   6s

NAME                                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/keda-admission-webhooks           0/1     1            0           6s
deployment.apps/keda-operator                     0/1     1            0           6s
deployment.apps/keda-operator-metrics-apiserver   0/1     1            0           6s

NAME                                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/keda-admission-webhooks-68cf687cbf           1         1         0       6s
replicaset.apps/keda-operator-656478d687                     1         1         0       6s
replicaset.apps/keda-operator-metrics-apiserver-7fd585f657   1         1         0       6s
$kubectl get crd | grep keda
clustertriggerauthentications.keda.sh        2023-05-21T13:45:20Z
scaledjobs.keda.sh                           2023-05-21T13:45:20Z
scaledobjects.keda.sh                        2023-05-21T13:45:20Z
triggerauthentications.keda.sh               2023-05-21T13:45:20Z
```

í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´, ìœ„ì—ì„œë„ ì‚¬ìš©í–ˆë˜ php-apache ë°°í¬

```bash
# í…ŒìŠ¤íŠ¸ìš© ë””í”Œë¡œì´ ë°°í¬
$kubectl apply -f php-apache.yaml -n keda
deployment.apps/php-apache created
service/php-apache created

# php-apache ë°°í¬ í™•ì¸
$kubectl get pod -n keda
NAME                                               READY   STATUS              RESTARTS      AGE
keda-admission-webhooks-68cf687cbf-l6lpx           1/1     Running             0             64s
keda-operator-656478d687-4m47m                     1/1     Running             1 (51s ago)   64s
keda-operator-metrics-apiserver-7fd585f657-xjltw   1/1     Running             0             64s
php-apache-698db99f59-rkqxp                        0/1     ContainerCreating   0             1s

```

íŠ¹ì •ì‹œê°„(0,15,30,45)ì— ì‹œì‘í•˜ê³  (05,20,35,50)ì¢…ë£Œí•˜ëŠ” ì´ë²¤íŠ¸ ê¸°ë°˜ ì •ì±… ìƒì„±

```bash
# ScaledObject ì •ì±… ìƒì„± : cron
# triggers ë¶€ë¶„ì—ì„œ ì´ë²¤íŠ¸ê¸°ë°˜ ì •ì±…ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.
$cat <<EOT > keda-cron.yaml
> apiVersion: keda.sh/v1alpha1
> kind: ScaledObject
> metadata:
>   name: php-apache-cron-scaled
> spec:
>   minReplicaCount: 0
>   maxReplicaCount: 2
>   pollingInterval: 30
>   cooldownPeriod: 300
>   scaleTargetRef:
>     apiVersion: apps/v1
>     kind: Deployment
>     name: php-apache
>   triggers:
>   - type: cron
>     metadata:
>       timezone: Asia/Seoul
>       start: 00,15,30,45 * * * *
>       end: 05,20,35,50 * * * *
>       desiredReplicas: "1"
> EOT

$kubectl apply -f keda-cron.yaml -n keda
scaledobject.keda.sh/php-apache-cron-scaled created

$k get ScaledObject -n  keda
NAME                     SCALETARGETKIND      SCALETARGETNAME   MIN   MAX   TRIGGERS   AUTHENTICATION   READY   ACTIVE   FALLBACK   AGE
php-apache-cron-scaled   apps/v1.Deployment   php-apache        0     2     cron                        True    False    Unknown    6m39s

ì•„ë˜ì˜ ëª…ë ¹ì–´ê°€ ì˜ ì•ˆë¨¹ìŒ
# $kubectl get ScaledObject -w

# í™•ì¸! 
$kubectl get ScaledObject,hpa,pod -n keda
NAME                                          SCALETARGETKIND      SCALETARGETNAME   MIN   MAX   TRIGGERS   AUTHENTICATION   READY   ACTIVE   FALLBACK   AGE
scaledobject.keda.sh/php-apache-cron-scaled   apps/v1.Deployment   php-apache        0     2     cron                        True    True     Unknown    2m45s

NAME                                                                  REFERENCE               TARGETS             MINPODS   MAXPODS   REPLICAS   AGE
horizontalpodautoscaler.autoscaling/keda-hpa-php-apache-cron-scaled   Deployment/php-apache   <unknown>/1 (avg)   1         2         1          2m45s

NAME                                                   READY   STATUS    RESTARTS        AGE
pod/keda-admission-webhooks-68cf687cbf-l6lpx           1/1     Running   0               4m12s
pod/keda-operator-656478d687-4m47m                     1/1     Running   1 (3m59s ago)   4m12s
pod/keda-operator-metrics-apiserver-7fd585f657-xjltw   1/1     Running   0               4m12s
pod/php-apache-698db99f59-rkqxp                        1/1     Running   0               3m9s
# ìœ„ì—ì„œ ACTIVEì¸ ì´ìœ ëŠ” í˜„ì¬ ì‹œê°ì´ 45ë¶„

$date
Sun May 21 22:50:43 KST 2023

# 50ë¶„ì´ ë˜ì—ˆìœ¼ë‹ˆ ë‹¤ì‹œ ëª…ë ¹ ì‹¤í–‰, ì•„ë˜ì˜ ACTIVE ì†ì„±ì´ 'False'ë¡œ ë°”ë€œ! ã„´
$kubectl get ScaledObject,hpa,pod -n keda
NAME                                          SCALETARGETKIND      SCALETARGETNAME   MIN   MAX   TRIGGERS   AUTHENTICATION   READY   ACTIVE   FALLBACK   AGE
scaledobject.keda.sh/php-apache-cron-scaled   apps/v1.Deployment   php-apache        0     2     cron                        True    False    Unknown    4m15s

NAME                                                                  REFERENCE               TARGETS             MINPODS   MAXPODS   REPLICAS   AGE
horizontalpodautoscaler.autoscaling/keda-hpa-php-apache-cron-scaled   Deployment/php-apache   <unknown>/1 (avg)   1         2         1          4m16s

NAME                                                   READY   STATUS    RESTARTS        AGE
pod/keda-admission-webhooks-68cf687cbf-l6lpx           1/1     Running   0               5m43s
pod/keda-operator-656478d687-4m47m                     1/1     Running   1 (5m30s ago)   5m43s
pod/keda-operator-metrics-apiserver-7fd585f657-xjltw   1/1     Running   0               5m43s
pod/php-apache-698db99f59-rkqxp                        1/1     Running   0               4m40s
# false í™•ì¸ ê°€ëŠ¥!
$kubectl get ScaledObject,hpa,pod -n keda
NAME                                          SCALETARGETKIND      SCALETARGETNAME   MIN   MAX   TRIGGERS   AUTHENTICATION   READY   ACTIVE   FALLBACK   AGE
scaledobject.keda.sh/php-apache-cron-scaled   apps/v1.Deployment   php-apache        0     2     cron                        True    False    Unknown    4m47s

NAME                                                                  REFERENCE               TARGETS             MINPODS   MAXPODS   REPLICAS   AGE
horizontalpodautoscaler.autoscaling/keda-hpa-php-apache-cron-scaled   Deployment/php-apache   <unknown>/1 (avg)   1         2         1          4m47s

NAME                                                   READY   STATUS    RESTARTS       AGE
pod/keda-admission-webhooks-68cf687cbf-l6lpx           1/1     Running   0              6m14s
pod/keda-operator-656478d687-4m47m                     1/1     Running   1 (6m1s ago)   6m14s
pod/keda-operator-metrics-apiserver-7fd585f657-xjltw   1/1     Running   0              6m14s
pod/php-apache-698db99f59-rkqxp                        1/1     Running   0              5m11s
$k get ScaledObject
No resources found in default namespace.
$kubectl get pod -n keda
NAME                                               READY   STATUS    RESTARTS        AGE
keda-admission-webhooks-68cf687cbf-l6lpx           1/1     Running   0               8m32s
keda-operator-656478d687-4m47m                     1/1     Running   1 (8m19s ago)   8m32s
keda-operator-metrics-apiserver-7fd585f657-xjltw   1/1     Running   0               8m32s
php-apache-698db99f59-rkqxp                        1/1     Running   0               7m29s

# ScaledObject í™•ì¸ ê°€ëŠ¥! 
$kubectl describe -n keda ScaledObject
Name:         php-apache-cron-scaled
Namespace:    keda
Labels:       scaledobject.keda.sh/name=php-apache-cron-scaled
Annotations:  <none>
API Version:  keda.sh/v1alpha1
Kind:         ScaledObject
Metadata:
  Creation Timestamp:  2023-05-21T13:46:48Z
  Finalizers:
    finalizer.keda.sh
  Generation:  1
  Managed Fields:
    API Version:  keda.sh/v1alpha1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:finalizers:
          .:
          v:"finalizer.keda.sh":
        f:labels:
          .:
          f:scaledobject.keda.sh/name:
    Manager:      keda
    Operation:    Update
    Time:         2023-05-21T13:46:48Z
    API Version:  keda.sh/v1alpha1
    Fields Type:  FieldsV1
    fieldsV1:
      f:status:
        .:
        f:conditions:
        f:externalMetricNames:
        f:hpaName:
        f:lastActiveTime:
        f:originalReplicaCount:
        f:scaleTargetGVKR:
          .:
          f:group:
          f:kind:
          f:resource:
          f:version:
        f:scaleTargetKind:
    Manager:      keda
    Operation:    Update
    Subresource:  status
    Time:         2023-05-21T13:46:48Z
    API Version:  keda.sh/v1alpha1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .:
          f:kubectl.kubernetes.io/last-applied-configuration:
      f:spec:
        .:
        f:cooldownPeriod:
        f:maxReplicaCount:
        f:minReplicaCount:
        f:pollingInterval:
        f:scaleTargetRef:
          .:
          f:apiVersion:
          f:kind:
          f:name:
        f:triggers:
    Manager:         kubectl-client-side-apply
    Operation:       Update
    Time:            2023-05-21T13:46:48Z
  Resource Version:  10339
  UID:               ac336308-4464-4c86-87d8-307de28d4df5
Spec:
  Cooldown Period:    300
  Max Replica Count:  2
  Min Replica Count:  0
  Polling Interval:   30
  Scale Target Ref:
    API Version:  apps/v1
    Kind:         Deployment
    Name:         php-apache
  Triggers:
    Metadata:
      Desired Replicas:  1
      End:               05,20,35,50 * * * *
      Start:             00,15,30,45 * * * *
      Timezone:          Asia/Seoul
    Type:                cron
Status:
  Conditions:
    Message:  ScaledObject is defined correctly and is ready for scaling
    Reason:   ScaledObjectReady
    Status:   True
    Type:     Ready
    Message:  Scaler cooling down because triggers are not active
    Reason:   ScalerCooldown
    Status:   False
    Type:     Active
    Status:   Unknown
    Type:     Fallback
  External Metric Names:
    s0-cron-Asia-Seoul-00,15,30,45xxxx-05,20,35,50xxxx
  Hpa Name:                keda-hpa-php-apache-cron-scaled
  Last Active Time:        2023-05-21T13:49:48Z
  Original Replica Count:  1
  Scale Target GVKR:
    Group:            apps
    Kind:             Deployment
    Resource:         deployments
    Version:          v1
  Scale Target Kind:  apps/v1.Deployment
Events:
  Type    Reason              Age    From           Message
  ----    ------              ----   ----           -------
  Normal  KEDAScalersStarted  7m56s  keda-operator  Started scalers watch
  Normal  ScaledObjectReady   7m56s  keda-operator  ScaledObject is ready for scaling
$date
Sun May 21 22:55:17 KST 2023

```

keda-operator ìì„¸íˆ í™•ì¸!

```bash

$k -n keda describe pod keda-operator
Name:             keda-operator-656478d687-4m47m
Namespace:        keda
Priority:         0
Service Account:  keda-operator
Node:             ip-192-168-3-10.ap-northeast-2.compute.internal/192.168.3.10
Start Time:       Sun, 21 May 2023 22:45:21 +0900
Labels:           app=keda-operator
                  app.kubernetes.io/component=operator
                  app.kubernetes.io/instance=keda
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=keda-operator
                  app.kubernetes.io/part-of=keda-operator
                  app.kubernetes.io/version=2.10.1
                  helm.sh/chart=keda-2.10.2
                  name=keda-operator
                  pod-template-hash=656478d687
Annotations:      container.seccomp.security.alpha.kubernetes.io/keda-operator: runtime/default
                  kubernetes.io/psp: eks.privileged
Status:           Running
IP:               192.168.3.180
IPs:
  IP:           192.168.3.180
Controlled By:  ReplicaSet/keda-operator-656478d687
Containers:
  keda-operator:
    Container ID:  containerd://e2fc885cb68003dce46ae15bb58f1b2156a4389bb55c6961cde0182fae232697
    Image:         ghcr.io/kedacore/keda:2.10.1
    Image ID:      ghcr.io/kedacore/keda@sha256:1489b706aa959a07765510edb579af34fa72636a26cfb755544c0ef776f3addf
    Port:          8080/TCP
    Host Port:     0/TCP
    Command:
      /keda
    Args:
      --leader-elect
      --zap-log-level=info
      --zap-encoder=console
      --zap-time-encoding=rfc3339
      --cert-dir=/certs
      --enable-cert-rotation=true
      --cert-secret-name=kedaorg-certs
      --operator-service-name=keda-operator
      --metrics-server-service-name=keda-operator-metrics-apiserver
      --webhooks-service-name=keda-admission-webhooks
      --metrics-bind-address=:8080
    State:          Running
      Started:      Sun, 21 May 2023 22:45:36 +0900
    Last State:     Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sun, 21 May 2023 22:45:30 +0900
      Finished:     Sun, 21 May 2023 22:45:34 +0900
    Ready:          True
    Restart Count:  1
    Limits:
      cpu:     1
      memory:  1000Mi
    Requests:
      cpu:      100m
      memory:   100Mi
    Liveness:   http-get http://:8081/healthz delay=25s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get http://:8081/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:
      WATCH_NAMESPACE:
      POD_NAME:                   keda-operator-656478d687-4m47m (v1:metadata.name)
      POD_NAMESPACE:              keda (v1:metadata.namespace)
      OPERATOR_NAME:              keda-operator
      KEDA_HTTP_DEFAULT_TIMEOUT:  3000
      KEDA_HTTP_MIN_TLS_VERSION:  TLS12
    Mounts:
      /certs from certificates (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5ktsx (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  certificates:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kedaorg-certs
    Optional:    true
  kube-api-access-5ktsx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason       Age                From               Message
  ----     ------       ----               ----               -------
  Normal   Scheduled    10m                default-scheduler  Successfully assigned keda/keda-operator-656478d687-4m47m to ip-192-168-3-10.ap-northeast-2.compute.internal
  Warning  FailedMount  10m                kubelet            MountVolume.SetUp failed for volume "certificates" : failed to sync secret cache: timed out waiting for the condition
  Normal   Pulled       10m                kubelet            Successfully pulled image "ghcr.io/kedacore/keda:2.10.1" in 6.362549579s
  Normal   Pulling      10m (x2 over 10m)  kubelet            Pulling image "ghcr.io/kedacore/keda:2.10.1"
  Normal   Created      10m (x2 over 10m)  kubelet            Created container keda-operator
  Normal   Started      10m (x2 over 10m)  kubelet            Started container keda-operator
  Normal   Pulled       10m                kubelet            Successfully pulled image "ghcr.io/kedacore/keda:2.10.1" in 699.131003ms

Name:             keda-operator-metrics-apiserver-7fd585f657-xjltw
Namespace:        keda
Priority:         0
Service Account:  keda-operator
Node:             ip-192-168-3-10.ap-northeast-2.compute.internal/192.168.3.10
Start Time:       Sun, 21 May 2023 22:45:21 +0900
Labels:           app=keda-operator-metrics-apiserver
                  app.kubernetes.io/component=operator
                  app.kubernetes.io/instance=keda
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=keda-operator-metrics-apiserver
                  app.kubernetes.io/part-of=keda-operator
                  app.kubernetes.io/version=2.10.1
                  helm.sh/chart=keda-2.10.2
                  pod-template-hash=7fd585f657
Annotations:      container.seccomp.security.alpha.kubernetes.io/keda-operator-metrics-apiserver: runtime/default
                  kubernetes.io/psp: eks.privileged
Status:           Running
IP:               192.168.3.10
IPs:
  IP:           192.168.3.10
Controlled By:  ReplicaSet/keda-operator-metrics-apiserver-7fd585f657
Containers:
  keda-operator-metrics-apiserver:
    Container ID:  containerd://8eca27ff65b4e907ece4d4fe53b8b85920a1023ca070dc806f0c32be612134df
    Image:         ghcr.io/kedacore/keda-metrics-apiserver:2.10.1
    Image ID:      ghcr.io/kedacore/keda-metrics-apiserver@sha256:d1f1ccc8d14e33ee448ec0c820f65b8a3e01b2dad23d9fa38fa7204a6c0194ca
    Ports:         6443/TCP, 8080/TCP, 9022/TCP
    Host Ports:    6443/TCP, 8080/TCP, 9022/TCP
    Args:
      /usr/local/bin/keda-adapter
      --port=8080
      --secure-port=6443
      --logtostderr=true
      --metrics-service-address=keda-operator.keda.svc.cluster.local:9666
      --client-ca-file=/certs/ca.crt
      --tls-cert-file=/certs/tls.crt
      --tls-private-key-file=/certs/tls.key
      --cert-dir=/certs
      --metrics-port=9022
      --metrics-path=/metrics
      --v=0
    State:          Running
      Started:      Sun, 21 May 2023 22:45:46 +0900
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1
      memory:  1000Mi
    Requests:
      cpu:      100m
      memory:   100Mi
    Liveness:   http-get https://:6443/healthz delay=5s timeout=1s period=10s #success=1 #failure=3
    Readiness:  http-get https://:6443/readyz delay=5s timeout=1s period=10s #success=1 #failure=3
    Environment:
      WATCH_NAMESPACE:
      POD_NAMESPACE:              keda (v1:metadata.namespace)
      KEDA_HTTP_DEFAULT_TIMEOUT:  3000
      KEDA_HTTP_MIN_TLS_VERSION:  TLS12
    Mounts:
      /certs from certificates (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-psqzd (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  certificates:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kedaorg-certs
    Optional:    false
  kube-api-access-psqzd:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason       Age                From               Message
  ----     ------       ----               ----               -------
  Normal   Scheduled    10m                default-scheduler  Successfully assigned keda/keda-operator-metrics-apiserver-7fd585f657-xjltw to ip-192-168-3-10.ap-northeast-2.compute.internal
  Warning  FailedMount  10m                kubelet            MountVolume.SetUp failed for volume "certificates" : failed to sync secret cache: timed out waiting for the condition
  Warning  FailedMount  10m (x4 over 10m)  kubelet            MountVolume.SetUp failed for volume "certificates" : secret "kedaorg-certs" not found
  Normal   Pulling      10m                kubelet            Pulling image "ghcr.io/kedacore/keda-metrics-apiserver:2.10.1"
  Normal   Pulled       10m                kubelet            Successfully pulled image "ghcr.io/kedacore/keda-metrics-apiserver:2.10.1" in 6.593596109s
  Normal   Created      10m                kubelet            Created container keda-operator-metrics-apiserver
  Normal   Started      10m                kubelet            Started container keda-operator-metrics-apiserver

```
![](https://velog.velcdn.com/images/han-0315/post/5c755392-ebb5-4a93-b56d-c623a9c712dc/image.png)


ê²°ê³¼ê°€ ì§„í–‰ëœ ê·¸ë¼íŒŒë‚˜ ëŒ€ì‹œë³´ë“œ ì‚¬ì§„(50ë¶„ì „ì— ì‹œì‘í•˜ê³ ,,, 55ë¶„ì— ëë‚˜ëŠ” ì¡°ê¸ˆ ì´ìƒí•˜ì§€ë§Œ?!) ì•”íŠ¼ ì´ë²¤íŠ¸ê¸°ë°˜ìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§ëœë‹¤!

![](https://velog.velcdn.com/images/han-0315/post/31fb45aa-fb3a-4c93-b9d2-c4a5eb4fbbbf/image.png)


![](https://velog.velcdn.com/images/han-0315/post/80a96400-ef13-4954-9e8e-662eb31b156d/image.png)

`[ë„ì „ê³¼ì œ2]` KEDA í™œìš© : Karpenter + KEDAë¡œ íŠ¹ì • ì‹œê°„ì— AutoScaling - [ë§í¬](https://jenakim47.tistory.com/90) [Youtube](https://youtu.be/FPlCVVrCD64) [Airflow](https://swalloow.github.io/airflow-worker-keda-autoscaler/)

â†’ ì€í–‰ì˜ ì›”ìš”ì¼ì•„ì¹¨, ì´ë²¤íŠ¸ ì¶”ì²¨ì‹œê°„ ë“± ëŒ€ê·œëª¨ íŠ¸ë˜í”½ì´ ì˜ˆìƒë˜ëŠ” ì‹œê°„ì— KEDAë¥¼ í†µí•´, ì˜¤í† ìŠ¤ì¼€ì¼ë§

## CA - Cluster Autoscaler(CAS)

CAëŠ” ë…¸ë“œì˜ ë¦¬ì†ŒìŠ¤ë¥¼ íŒŒì•…í•˜ê³ , ë¦¬ì†ŒìŠ¤ ë©”íŠ¸ë¦­ì— ì˜í•´ íŠ¸ë¦¬ê±°ë˜ì–´ ë…¸ë“œë¥¼ ìŠ¤ì¼€ì¼ë§í•œë‹¤. ì˜¤í† ìŠ¤ì¼€ì¼ëŸ¬ ë™ì‘ì„ ìœ„í•´ ë³„ë„ì˜ ë””í”Œë¡œì´ë¥¼ ë°°í¬í•´ë‘”ë‹¤. CAëŠ” `pending` ìƒíƒœì˜ íŒŒë“œê°€ ì¡´ì¬í•  ê²½ìš°, ì›Œì»¤ë…¸ë“œë¥¼ ìŠ¤ì¼€ì¼ ì•„ì›ƒí•œë‹¤.

![](https://velog.velcdn.com/images/han-0315/post/2a067472-4819-4d25-9124-a8ed2c24a7c7/image.png)


í´ë¼ìš°ë“œ í”Œë«í¼ì—ì„œ ì£¼ë¡œ ì‚¬ìš©í•˜ê³ , ì‹¤ì œ ë…¸ë“œ ë¦¬ì†ŒìŠ¤ê°€ ì—†ëŠ” ë° íŒŒë“œë¥¼ ë°°í¬í•´ì•¼ í•˜ë©´ ì‹ ê·œ ì›Œì»¤ë…¸ë“œë¥¼ ì¶”ê°€í•œë‹¤. ì˜¨í”„ë¼ë¯¸ìŠ¤ì—ì„  ì ìš©í•˜ê¸´ í˜ë“¤ë‹¤.

**ì‹¤ìŠµì§„í–‰**

ì¼ë¶€ë¡œ ì›Œì»¤ë…¸ë“œê°€ ê°ë‹¹ëª»í•˜ê²Œ requestë¥¼ 500m = 0.5 ì½”ì–´ë¡œ í• ë‹¹ â†’ ìŠ¤ì¼€ì¼ë§ì„ í†µí•´ 15ê°œë¡œ ëŠ˜ë¦¼ â†’ ë…¸ë“œ ë¶€ì¡±!

```bash
$**aws autoscaling describe-auto-scaling-groups --query "AutoScalingGroups[? Tags[? (Key=='eks:cluster-name') && Value=='myeks']].[AutoScalingGroupName, MinSize, MaxSize,DesiredCapacity]" --output table**
-----------------------------------------------------------------
|                   DescribeAutoScalingGroups                   |
+------------------------------------------------+----+----+----+
|  eks-ng1-c2c41e26-6213-a429-9a58-02374389d5c3  |  3 |  6 |  3 |
+------------------------------------------------+----+----+----+
ì•„ë˜ëŠ” min, max, desired
```

```bash
# autuscaler ê¶Œí•œ í™•ì¸(ê´€ë ¨ íƒœê·¸)
$aws ec2 describe-instances  --filters Name=tag:Name,Values=$CLUSTER_NAME-ng1-Node --query "Reservations[*].Instances[*].Tags[*]" --output yaml | yh | grep autoscaler
    - Key: k8s.io/cluster-autoscaler/enabled
    - Key: k8s.io/cluster-autoscaler/myeks
    - Key: k8s.io/cluster-autoscaler/enabled
    - Key: k8s.io/cluster-autoscaler/myeks
    - Key: k8s.io/cluster-autoscaler/myeks
    - Key: k8s.io/cluster-autoscaler/enabled
# ì•„ë˜ì˜ í…Œì´ë¸”ì€ ì°¨ë¡€ë¡œ, (min,max,desired)ì„ ì˜ë¯¸!
$aws autoscaling describe-auto-scaling-groups \
>     --query "AutoScalingGroups[? Tags[? (Key=='eks:cluster-name') && Value=='myeks']].[AutoScalingGroupName, MinSize, MaxSize,DesiredCapacity]" \
>     --output table
-----------------------------------------------------------------
|                   DescribeAutoScalingGroups                   |
+------------------------------------------------+----+----+----+
|  eks-ng1-cec41f38-acab-b45a-0479-ca4ecb1586cc  |  3 |  3 |  3 |
+------------------------------------------------+----+----+----+

$export ASG_NAME=$(aws autoscaling describe-auto-scaling-groups --query "AutoScalingGroups[? Tay=='eks:cluster-name') && Value=='myeks']].AutoScalingGroupName" --output text)
# MaxSize 6ê°œë¡œ ìˆ˜ì •
$aws autoscaling update-auto-scaling-group --auto-scaling-group-name ${ASG_NAME} --min-size 3 --desired-capacity 3 --max-size 6
# ì •ë³´ í™•ì¸
$aws autoscaling describe-auto-scaling-groups --query "AutoScalingGroups[? Tags[? (Key=='eks:cluster-name') && Value=='myeks']].[AutoScalingGroupName, MinSize, MaxSize,DesiredCapacity]" --output table
-----------------------------------------------------------------
|                   DescribeAutoScalingGroups                   |
+------------------------------------------------+----+----+----+
|  eks-ng1-cec41f38-acab-b45a-0479-ca4ecb1586cc  |  3 |  6 |  3 |
+------------------------------------------------+----+----+----+

# ë°°í¬ : Deploy the Cluster Autoscaler (CA)
$curl -s -O https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml
$sed -i "s/<YOUR CLUSTER NAME>/$CLUSTER_NAME/g" cluster-autoscaler-autodiscover.yaml
$kubectl apply -f cluster-autoscaler-autodiscover.yaml
serviceaccount/cluster-autoscaler created
clusterrole.rbac.authorization.k8s.io/cluster-autoscaler created
role.rbac.authorization.k8s.io/cluster-autoscaler created
clusterrolebinding.rbac.authorization.k8s.io/cluster-autoscaler created
rolebinding.rbac.authorization.k8s.io/cluster-autoscaler created
deployment.apps/cluster-autoscaler created
$kubectl get pod -n kube-system | grep cluster-autoscaler
cluster-autoscaler-74785c8d45-wrtjp             0/1     ContainerCreating   0          7s
$kubectl describe deployments.apps -n kube-system cluster-autoscaler
Name:                   cluster-autoscaler
Namespace:              kube-system
CreationTimestamp:      Sun, 21 May 2023 23:16:38 +0900
Labels:                 app=cluster-autoscaler
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=cluster-autoscaler
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:           app=cluster-autoscaler
  Annotations:      prometheus.io/port: 8085
                    prometheus.io/scrape: true
  Service Account:  cluster-autoscaler
  Containers:
   cluster-autoscaler:
    Image:      registry.k8s.io/autoscaling/cluster-autoscaler:v1.26.2
    Port:       <none>
    Host Port:  <none>
    Command:
      ./cluster-autoscaler
      --v=4
      --stderrthreshold=info
      --cloud-provider=aws
      --skip-nodes-with-local-storage=false
      --expander=least-waste
      --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/myeks
    Limits:
      cpu:     100m
      memory:  600Mi
    Requests:
      cpu:        100m
      memory:     600Mi
    Environment:  <none>
    Mounts:
      /etc/ssl/certs/ca-certificates.crt from ssl-certs (ro)
  Volumes:
   ssl-certs:
    Type:               HostPath (bare host directory volume)
    Path:               /etc/ssl/certs/ca-bundle.crt
    HostPathType:
  Priority Class Name:  system-cluster-critical
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   cluster-autoscaler-74785c8d45 (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  18s   deployment-controller  Scaled up replica set cluster-autoscaler-74785c8d45 to 1
$kubectl -n kube-system annotate deployment.apps/cluster-autoscaler cluster-autoscaler.kubernetes.io/safe-to-evict="false"^C
$kubectl -n kube-system annotate deployment.apps/cluster-autoscaler cluster-autoscaler.kubernetes.io/safe-to-evict="false"^C
# (ì˜µì…˜) cluster-autoscaler íŒŒë“œê°€ ë™ì‘í•˜ëŠ” ì›Œì»¤ ë…¸ë“œê°€ í‡´ì¶œ(evict) ë˜ì§€ ì•Šê²Œ ì„¤ì •
$kubectl -n kube-system annotate deployment.apps/cluster-autoscaler cluster-autoscaler.kubernetes.io/safe-to-evict="false"
deployment.apps/cluster-autoscaler annotated
```

í…ŒìŠ¤íŠ¸ ì§„í–‰, íŒŒë“œ í•˜ë‚˜ë‹¹ ë¦¬ì†ŒìŠ¤ ìš”ì²­ëŸ‰ì„ í¬ê²Œ ì¡ê³ , ìŠ¤ì¼€ì¼ì•„ì›ƒí•˜ì—¬ ì˜¤í† ìŠ¤ì¼€ì¼ë§ì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™ë˜ë‚˜ í™•ì¸

```bash
$cat <<EoF> nginx.yaml
> apiVersion: apps/v1
> kind: Deployment
> metadata:
>   name: nginx-to-scaleout
> spec:
>   replicas: 1
>   selector:
>     matchLabels:
>       app: nginx
>   template:
>     metadata:
>       labels:
>         service: nginx
>         app: nginx
>     spec:
>       containers:
>       - image: nginx
>         name: nginx-to-scaleout
>         resources:
>           limits:
>             cpu: 500m
>             memory: 512Mi
>           requests:
>             cpu: 500m
>             memory: 512Mi
> EoF
$kubectl apply -f nginx.yaml
deployment.apps/nginx-to-scaleout created
$kubectl get deployment/nginx-to-scaleout
NAME                READY   UP-TO-DATE   AVAILABLE   AGE
nginx-to-scaleout   0/1     1            0           3s

# íŒŒë“œ ê°œìˆ˜ ì¦ê°€
$kubectl scale --replicas=15 deployment/nginx-to-scaleout && date
deployment.apps/nginx-to-scaleout scaled
Sun May 21 23:18:42 KST 2023
$kubectl get pods -l app=nginx -o wide --watch
NAME                                 READY   STATUS    RESTARTS   AGE    IP              NODE                                               NOMINATED NODE   READINESS GATES
nginx-to-scaleout-79df8996f6-2vjqm   1/1     Running   0          82s    192.168.3.11    ip-192-168-3-6.ap-northeast-2.compute.internal     <none>           <none>
nginx-to-scaleout-79df8996f6-5pc5k   1/1     Running   0          82s    192.168.3.221   ip-192-168-3-6.ap-northeast-2.compute.internal     <none>           <none>
nginx-to-scaleout-79df8996f6-5wvgl   1/1     Running   0          82s    192.168.2.172   ip-192-168-2-104.ap-northeast-2.compute.internal   <none>           <none>
nginx-to-scaleout-79df8996f6-8bbc9   1/1     Running   0          82s    192.168.3.188   ip-192-168-3-10.ap-northeast-2.compute.internal    <none>           <none>
nginx-to-scaleout-79df8996f6-9j5rx   1/1     Running   0          2m4s   192.168.3.88    ip-192-168-3-10.ap-northeast-2.compute.internal    <none>           <none>
nginx-to-scaleout-79df8996f6-9kks9   1/1     Running   0          82s    192.168.1.166   ip-192-168-1-146.ap-northeast-2.compute.internal   <none>           <none>
nginx-to-scaleout-79df8996f6-b5qmk   1/1     Running   0          82s    192.168.2.32    ip-192-168-2-104.ap-northeast-2.compute.internal   <none>           <none>
nginx-to-scaleout-79df8996f6-c5hhq   1/1     Running   0          82s    192.168.1.127   ip-192-168-1-189.ap-northeast-2.compute.internal   <none>           <none>
nginx-to-scaleout-79df8996f6-dzwsn   1/1     Running   0          82s    192.168.1.10    ip-192-168-1-146.ap-northeast-2.compute.internal   <none>           <none>
nginx-to-scaleout-79df8996f6-nlnjq   1/1     Running   0          82s    192.168.2.253   ip-192-168-2-104.ap-northeast-2.compute.internal   <none>           <none>
nginx-to-scaleout-79df8996f6-qmg8v   1/1     Running   0          82s    192.168.1.5     ip-192-168-1-189.ap-northeast-2.compute.internal   <none>           <none>
nginx-to-scaleout-79df8996f6-rn7sw   1/1     Running   0          82s    192.168.1.130   ip-192-168-1-146.ap-northeast-2.compute.internal   <none>           <none>
nginx-to-scaleout-79df8996f6-sd4rs   1/1     Running   0          82s    192.168.1.249   ip-192-168-1-189.ap-northeast-2.compute.internal   <none>           <none>
nginx-to-scaleout-79df8996f6-wrw9d   1/1     Running   0          82s    192.168.3.91    ip-192-168-3-10.ap-northeast-2.compute.internal    <none>           <none>
nginx-to-scaleout-79df8996f6-wwz6f   1/1     Running   0          82s    192.168.3.121   ip-192-168-3-6.ap-northeast-2.compute.internal     <none>           <none>
# ë…¸ë“œê°€ ì¶”ê°€ì ìœ¼ë¡œ ë¶™ëŠ” ëª¨ìŠµ, ê´€ë ¨ëœ eks-node-view, ê·¸ë¼íŒŒë‚˜ ì‚¬ì§„ë„ ì•„ë˜ì— ìˆìŠµë‹ˆë‹¤.
$kubectl get nodes
NAME                                               STATUS   ROLES    AGE   VERSION
ip-192-168-1-146.ap-northeast-2.compute.internal   Ready    <none>   38s   v1.24.13-eks-0a21954
ip-192-168-1-189.ap-northeast-2.compute.internal   Ready    <none>   65m   v1.24.13-eks-0a21954
ip-192-168-2-104.ap-northeast-2.compute.internal   Ready    <none>   65m   v1.24.13-eks-0a21954
ip-192-168-3-10.ap-northeast-2.compute.internal    Ready    <none>   65m   v1.24.13-eks-0a21954
ip-192-168-3-6.ap-northeast-2.compute.internal     Ready    <none>   37s   v1.24.13-eks-0a21954
# í…Œì´ë¸”ë¡œ í™•ì¸
$aws autoscaling describe-auto-scaling-groups \
>     --query "AutoScalingGroups[? Tags[? (Key=='eks:cluster-name') && Value=='myeks']].[AutoScalingGroupName, MinSize, MaxSize,DesiredCapacity]" \
>     --output table
-----------------------------------------------------------------
|                   DescribeAutoScalingGroups                   |
+------------------------------------------------+----+----+----+
|  eks-ng1-cec41f38-acab-b45a-0479-ca4ecb1586cc  |  3 |  6 |  5 |
+------------------------------------------------+----+----+----+
$kubectl delete -f nginx.yaml && date
deployment.apps "nginx-to-scaleout" deleted
Sun May 21 23:21:27 KST 2023

# ìë™ìœ¼ë¡œ ì‚­ì œë˜ëŠ” ë°, ì‹œê°„ì´ 10ë¶„ì´ìƒ ì§€ì²´ë˜ì–´ ì‚­ì œ!
```

íŒŒë“œì˜ ê°œìˆ˜ë¥¼ ì¦ê°€í•˜ì, CPU ì‚¬ìš©ëŸ‰ í­ì¦

![](https://velog.velcdn.com/images/han-0315/post/2d1b66bf-ac12-45ce-aecc-62d2d2a97a07/image.png)


kube-ops-viewì—ì„œë„ í• ë‹¹ëª»ë°›ëŠ” íŒŒë“œë¥¼ í™•ì¸ê°€ëŠ¥

![](https://velog.velcdn.com/images/han-0315/post/a96d567c-7c7d-469a-9f0c-c717ccd2ada9/image.png)

ë…¸ë“œê°€ ì˜¤í† ìŠ¤ì¼€ì¼ë§ë˜ì–´, ì¦ê°€ë˜ëŠ” ê²ƒì„ í™•ì¸ê°€ëŠ¥!

![](https://velog.velcdn.com/images/han-0315/post/40569b75-e527-4685-9eea-adc47d4d9d11/image.png)

ì‚­ì œ í›„ ì‚¬ìš©ëŸ‰ì´ ì‘ì•„ì§„ ëª¨ìŠµ

![](https://velog.velcdn.com/images/han-0315/post/7034a309-2321-41fd-b4a7-8969e3f432e8/image.png)

ë°ëª¬ì…‹ë„ ì ê³ , ì—¬ëŸ¬ ê°€ì§€ ë…¸ë“œì˜ ê¸°ë³¸ì ì¸ ì„¸íŒ…ì´ ë³„ë¡œ ì—†ì–´ì„œ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¹ ë¥´ì§€ë§Œ, ì‹¤ì œ ìš´ì˜í™˜ê²½ì—ì„œëŠ” ë¹ ë¥´ì§€ ì•ŠìŒ! â†’ ìë™ìœ¼ë¡œ ì¶•ì†Œí•˜ëŠ” 10ë¶„ì •ë„ë¡œ ì‹œê°„ë„ ëŠë¦¬ë‹¤. (ì‹¤ì œ í™˜ê²½ì´ë©´ ê¸°ë³¸ì ìœ¼ë¡œ ë°°í¬í•˜ëŠ” ê²ƒë“¤ì´ ë§ì•„ ë” ëŠë¦¬ë‹¤ê³  í•œë‹¤.)

## CPA - Cluster Proportional Autoscaler

coredns íŒŒë“œì™€ ê°™ì´ í´ëŸ¬ìŠ¤í„°ê°€ ì»¤ì§ˆìˆ˜ë¡ ë¶€í•˜ê°€ ì¦ê°€í•˜ëŠ” ì¤‘ìš”í•œ íŒŒë“œëŠ” ë…¸ë“œì˜ ê°œìˆ˜ì— ë¹„ë¡€í•´ì„œ íŒŒë“œë¥¼ ì˜¤í† ìŠ¤ì¼€ì¼ë§í•œë‹¤. CPAëŠ” ë…¸ë“œ ë¹„ë¡€ íŒŒë“œ ìŠ¤ì¼€ì¼ë§ì´ë‹¤. ì§ì ‘ ê·œì¹™ì„ ë§Œë“¤ì–´ì„œ ì •ì±…ì„ ìˆ˜ë¦½í•œë‹¤. **Metrics serverÂ ë“±ì„Â ì‚¬ìš©í•˜ì§€Â ì•Šê³ Â kubapi server APIë¥¼Â ì‚¬ìš©**í•©ë‹ˆë‹¤.Â ì‚¬ìš©ì ì…ì¥ì—ì„œëŠ” ì ì ˆí•œ ê·œì¹™ë§Œ ì„¸ìš°ë©´ ëœë‹¤.!

ë°°í¬! 

```bash
$helm repo add cluster-proportional-autoscaler https://kubernetes-sigs.github.io/cluster-proportional-autoscaler
"cluster-proportional-autoscaler" already exists with the same configuration, skipping

# cluster-proportional-autoscaler ë¶€í„° ë°°í¬!
$helm upgrade --install cluster-proportional-autoscaler cluster-proportional-autoscaler/cluster-proportional-autoscaler
Release "cluster-proportional-autoscaler" does not exist. Installing it now.

$cat <<EOT > cpa-nginx.yaml
> apiVersion: apps/v1
> kind: Deployment
> metadata:
>   name: nginx-deployment
> spec:
>   replicas: 1
>   selector:
>     matchLabels:
>       app: nginx
>   template:
>     metadata:
>       labels:
>         app: nginx
>     spec:
>       containers:
>       - name: nginx
>         image: nginx:latest
>         resources:
>           limits:
>             cpu: "100m"
>             memory: "64Mi"
>           requests:
>             cpu: "100m"
>             memory: "64Mi"
>         ports:
>         - containerPort: 80
> EOT
$kubectl apply -f cpa-nginx.yaml

deployment.apps/nginx-deployment created

# ì•„ë˜ì˜ ê·œì¹™ í™•ì¸
$cat <<EOF > cpa-values.yaml
> config:
>   ladder:
>     nodesToReplicas:
>       - [1, 1]
>       - [2, 2]
>       - [3, 3]
>       - [4, 3]
>       - [5, 5]
> options:
>   namespace: default
>   target: "deployment/nginx-deployment"
> EOF

$helm upgrade --install cluster-proportional-autoscaler -f cpa-values.yaml cluster-proportional-autoscaler/cluster-proportional-autoscaler
NAME: cluster-proportional-autoscaler
LAST DEPLOYED: Sun May 21 23:57:59 2023
NAMESPACE: default
STATUS: deployed
REVISION: 1
TEST SUITE: None
autoscaler  awscliv2.zip    cpa-values.yaml  externaldns.yaml  irsa.yaml         krew-linux_amd64.tar.gz  kube-ps1  monitor-values.yaml  precmd.yaml
aws         cpa-nginx.yaml  create-eks.log   go                krew-linux_amd64  kubectl                  LICENSE   myeks.yaml           yh-linux-amd64.zip
addon-resizer  balancer  builder  charts  cluster-autoscaler  code-of-conduct.md  CONTRIBUTING.md  hack  LICENSE  OWNERS  README.md  SECURITY_CONTACTS  vertical-pod-autoscaler

$helm repo add cluster-proportional-autoscaler https://kubernetes-sigs.github.io/cluster-proportional-autoscaler
"cluster-proportional-autoscaler" already exists with the same configuration, skipping
$helm upgrade --install cluster-proportional-autoscaler cluster-proportional-autoscaler/cluster-proportional-autoscaler
Release "cluster-proportional-autoscaler" has been upgraded. Happy Helming!
NAME: cluster-proportional-autoscaler
LAST DEPLOYED: Sun May 21 23:59:35 2023
NAMESPACE: default
STATUS: deployed
REVISION: 2
TEST SUITE: None

$helm upgrade --install cluster-proportional-autoscaler -f cpa-values.yaml cluster-proportional-autoscaler/cluster-proportional-autoscaler
Release "cluster-proportional-autoscaler" has been upgraded. Happy Helming!
NAME: cluster-proportional-autoscaler
LAST DEPLOYED: Sun May 21 23:59:49 2023
NAMESPACE: default
STATUS: deployed
REVISION: 3
TEST SUITE: None

```

ì´ì œ ë…¸ë“œë¥¼ ì¦ê°€ì‹œì¼œ í…ŒìŠ¤íŠ¸! ì•„ë˜ëŠ” [ë…¸ë“œ, íŒŒë“œ] í…Œì´ë¸”ì´ë‹¤. 
`[1, 1], [2, 2], [3, 3], [4, 3], [5, 5]`

```bash

$export ASG_NAME=$(aws autoscaling describe-auto-scaling-groups --query "AutoScalingGroups[? Tags[? (Key=='eks:cluster-name') && Value=='myeks']].AutoScalingGroupName" --output text)
# ë…¸ë“œ 5ê°œë¡œ ì¦ê°€
$aws autoscaling update-auto-scaling-group --auto-scaling-group-name ${ASG_NAME} --min-size 5 --desired-capacity 5 --max-size 5
$aws autoscaling describe-auto-scaling-groups --query "AutoScalingGroups[? Tags[? (Key=='eks:cluster-name') && Value=='myeks']].[AutoScalingGroupName, MinSize, MaxSize,DesiredCapacity]" --output table
-----------------------------------------------------------------
|                   DescribeAutoScalingGroups                   |
+------------------------------------------------+----+----+----+
|  eks-ng1-cec41f38-acab-b45a-0479-ca4ecb1586cc  |  5 |  5 |  5 |
+------------------------------------------------+----+----+----+

# ë…¸ë“œ 3ê°œ
$k get no
NAME                                               STATUS   ROLES    AGE    VERSION
ip-192-168-1-146.ap-northeast-2.compute.internal   Ready    <none>   41m    v1.24.13-eks-0a21954
ip-192-168-2-104.ap-northeast-2.compute.internal   Ready    <none>   105m   v1.24.13-eks-0a21954
ip-192-168-3-6.ap-northeast-2.compute.internal     Ready    <none>   41m    v1.24.13-eks-0a21954

# íŒŒë“œ 3ê°œ, [3,3] ê·œì¹™ë§Œì¡±
$k get po
NAME                                               READY   STATUS    RESTARTS   AGE
cluster-proportional-autoscaler-75bddf49cb-lwhxs   1/1     Running   0          2m46s
nginx-deployment-858477475d-8lj5g                  1/1     Running   0          2m45s
nginx-deployment-858477475d-l8rsd                  1/1     Running   0          2m45s
nginx-deployment-858477475d-z8jbb                  1/1     Running   0          3m1s

# ë…¸ë“œë¥¼ 5ê°œë¡œ ìŠ¤ì¼€ì¼ë§, [5,5] ê·œì¹™í™•ì¸
$k get no
NAME                                               STATUS     ROLES    AGE    VERSION
ip-192-168-1-146.ap-northeast-2.compute.internal   Ready      <none>   41m    v1.24.13-eks-0a21954
ip-192-168-2-104.ap-northeast-2.compute.internal   Ready      <none>   106m   v1.24.13-eks-0a21954
ip-192-168-2-217.ap-northeast-2.compute.internal   Ready      <none>   17s    v1.24.13-eks-0a21954
ip-192-168-3-6.ap-northeast-2.compute.internal     Ready      <none>   41m    v1.24.13-eks-0a21954
ip-192-168-3-76.ap-northeast-2.compute.internal    NotReady   <none>   6s     v1.24.13-eks-0a21954

# íŒŒë“œê°€ 5ê°œë¡œ ë§Œì¡±í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŒ
$k get po
NAME                                               READY   STATUS    RESTARTS   AGE
cluster-proportional-autoscaler-75bddf49cb-lwhxs   1/1     Running   0          3m24s
nginx-deployment-858477475d-8lj5g                  1/1     Running   0          3m23s
nginx-deployment-858477475d-jv2z6                  1/1     Running   0          23s
nginx-deployment-858477475d-l8rsd                  1/1     Running   0          3m23s
nginx-deployment-858477475d-rpd2s                  1/1     Running   0          23s
nginx-deployment-858477475d-z8jbb                  1/1     Running   0          3m39s

# ë…¸ë“œë¥¼ 4ê°œë¡œ ìŠ¤ì¼€ì¼ë§
$aws autoscaling update-auto-scaling-group --auto-scaling-group-name ${ASG_NAME} --min-size 4 --desired-capacity 4 --max-size 4
# ë…¸ë“œ í•˜ë‚˜ë¡œ ì¤„ì–´ì„œ ì´ì œ íŒŒë“œë„ í•œê°œ ì¤„ì–´ì•¼í•¨
# ì£½ì§€ ì•Šê³ , ìŠ¤ì¼€ì¤„ë¶ˆê°€ ìƒíƒœë¡œ ë‘ë©´ ì•Œì•„ì„œ, [4,4] ê·œì¹™ ë§Œì¡±
$k get no
NAME                                               STATUS                     ROLES    AGE    VERSION
ip-192-168-1-146.ap-northeast-2.compute.internal   Ready                      <none>   42m    v1.24.13-eks-0a21954
ip-192-168-2-104.ap-northeast-2.compute.internal   Ready,SchedulingDisabled   <none>   107m   v1.24.13-eks-0a21954
ip-192-168-2-217.ap-northeast-2.compute.internal   Ready                      <none>   74s    v1.24.13-eks-0a21954
ip-192-168-3-6.ap-northeast-2.compute.internal     Ready                      <none>   42m    v1.24.13-eks-0a21954
ip-192-168-3-76.ap-northeast-2.compute.internal    Ready                      <none>   63s    v1.24.13-eks-0a21954
$k get po
NAME                                               READY   STATUS    RESTARTS   AGE
cluster-proportional-autoscaler-75bddf49cb-lwhxs   1/1     Running   0          4m6s
nginx-deployment-858477475d-jv2z6                  1/1     Running   0          65s
nginx-deployment-858477475d-l8rsd                  1/1     Running   0          4m5s
nginx-deployment-858477475d-z8jbb                  1/1     Running   0          4m21s
#  - [4, 3] í…Œì´ë¸” ê·œì¹™ ë§Œì¡±
$aws autoscaling describe-auto-scaling-groups --query "AutoScalingGroups[? Tags[? (Key=='eks:cluster-name') && Value=='myeks']].[AutoScalingGroupName, MinSize, MaxSize,DesiredCapacity]" --output table
-----------------------------------------------------------------
|                   DescribeAutoScalingGroups                   |
+------------------------------------------------+----+----+----+
|  eks-ng1-cec41f38-acab-b45a-0479-ca4ecb1586cc  |  4 |  4 |  4 |
+------------------------------------------------+----+----+----+
$helm uninstall cluster-proportional-autoscaler && kubectl delete -f cpa-nginx.yaml
release "cluster-proportional-autoscaler" uninstalled
deployment.apps "nginx-deployment" deleted

```

Karpenter ì‹¤ìŠµ í™˜ê²½ ì¤€ë¹„ë¥¼ ìœ„í•´ì„œ í˜„ì¬ EKS ì‹¤ìŠµ í™˜ê²½ ì „ë¶€ ì‚­ì œ

```bash
$helm uninstall -n kube-system kube-ops-view
release "kube-ops-view" uninstalled
$helm uninstall -n monitoring kube-prometheus-stack
release "kube-prometheus-stack" uninstalled
$eksctl delete cluster --name $CLUSTER_NAME && aws cloudformation delete-stack --stack-name $CLUSTER_NAME
```

## Karpenter : K8S Native AutoScaler & Fargate

KarpenterëŠ” ì˜¤í”ˆì†ŒìŠ¤ ë…¸ë“œ ìˆ˜ëª…ì£¼ê¸° ê´€ë¦¬ ì†”ë£¨ì…˜ìœ¼ë¡œ, ê¸°ì¡´ ì˜¤í† ìŠ¤ì¼€ì¼ë§ íˆ´ê³¼ëŠ” ë‹¤ë¥´ê²Œ ì´ˆë‹¨ìœ„ë¡œ ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ë¥¼ ì œê³µí•œë‹¤. ì•„ë˜ì˜ ê·¸ë¦¼ê³¼ ê°™ì´ ê¸°ì¡´ì˜ ì˜¤í† ìŠ¤ì¼€ì¼ë§ì—ì„œ ê±°ì¹˜ë˜ ë‹¨ê³„ë¥¼ ìƒëµí•˜ê¸°ì— ê°€ëŠ¥í•˜ë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ìŠ¤í„°ë””ì›ë¶„ì˜ ë¸”ë¡œê·¸ë¥¼ ì°¸ê³ í•˜ë©´ ëœë‹¤.

> **linuxer ì •íƒœí™˜**ë‹˜ì´ **EKS Nodeless ì»¨ì…‰**ì„ ì •ë¦¬í•´ì£¼ì…¨ë‹¤! [ë§í¬](https://verifa.io/blog/how-to-create-nodeless-aws-eks-clusters-with-karpenter/index.html)
> 

![](https://velog.velcdn.com/images/han-0315/post/febd6550-8ef4-44e0-8b39-91543f484d5d/image.png)


![](https://velog.velcdn.com/images/han-0315/post/ed1ae0a9-14e6-48b4-9273-8b2f7da891b3/image.png)

**ì‹œê°„ì´ ì–´ëŠì •ë„ ê±¸ë ¤ì„œ ë°°í¬ë¶€í„° ì§„í–‰ â†’ ì‘ì—…ìš© EC2ë§Œ ë§Œë“¤ì–´ì§, ìŠ¤íƒì´ë¦„ì„ ê¸°ì¡´ê³¼ ì¼ë¶€ë¡œ êµ¬ë¶„!**

```bash
# í™˜ê²½ ë³€ìˆ˜ í™•ì¸
$export | egrep 'ACCOUNT|AWS_|CLUSTER' | egrep -v 'SECRET|KEY'
declare -x ACCOUNT_ID="871103481195"
declare -x AWS_ACCOUNT_ID="871103481195"
declare -x AWS_DEFAULT_REGION="ap-northeast-2"
declare -x AWS_PAGER=""
declare -x AWS_REGION="ap-northeast-2"
declare -x CLUSTER_NAME="myeks2"

# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
$export KARPENTER_VERSION=v0.27.5
$export TEMPOUT=$(mktemp)
$echo $KARPENTER_VERSION $CLUSTER_NAME $AWS_DEFAULT_REGION $AWS_ACCOUNT_ID $TEMPOUT
v0.27.5 myeks2 ap-northeast-2 871103481195 /tmp/tmp.X5iRenSltU

# CloudFormation ìŠ¤íƒìœ¼ë¡œ IAM Policy, Role, EC2 Instance Profile ìƒì„± : 3ë¶„ ì •ë„ ì†Œìš”
$curl -fsSL https://karpenter.sh/"${KARPENTER_VERSION}"/getting-started/getting-started-with-karpenter/cloudformation.yaml  > $TEMPOUT \
> && aws cloudformation deploy \
>   --stack-name "Karpenter-${CLUSTER_NAME}" \
>   --template-file "${TEMPOUT}" \
>   --capabilities CAPABILITY_NAMED_IAM \
>   --parameter-overrides "ClusterName=${CLUSTER_NAME}"
Waiting for changeset to be created..
No changes to deploy. Stack Karpenter-myeks2 is up to date

```

**í´ëŸ¬ìŠ¤í„° ìƒì„±**

- ë°°í¬ íŒŒì¼
    
    ```bash
    eksctl create cluster -f - <<EOF
    ---
    apiVersion: eksctl.io/v1alpha5
    kind: ClusterConfig
    metadata:
      name: ${CLUSTER_NAME}
      region: ${AWS_DEFAULT_REGION}
      version: "1.24"
      tags:
        karpenter.sh/discovery: ${CLUSTER_NAME}
    
    iam:
      withOIDC: true
      serviceAccounts:
      - metadata:
          name: karpenter
          namespace: karpenter
        roleName: ${CLUSTER_NAME}-karpenter
        attachPolicyARNs:
        - arn:aws:iam::${AWS_ACCOUNT_ID}:policy/KarpenterControllerPolicy-${CLUSTER_NAME}
        roleOnly: true
    
    iamIdentityMappings:
    - arn: "arn:aws:iam::${AWS_ACCOUNT_ID}:role/KarpenterNodeRole-${CLUSTER_NAME}"
      username: system:node:{{EC2PrivateDNSName}}
      groups:
      - system:bootstrappers
      - system:nodes
    
    managedNodeGroups:
    - instanceType: m5.large
      amiFamily: AmazonLinux2
      name: ${CLUSTER_NAME}-ng
      desiredCapacity: 2
      minSize: 1
      maxSize: 10
      iam:
        withAddonPolicies:
          externalDNS: true
    
    ## Optionally run on fargate
    # fargateProfiles:
    # - name: karpenter
    #  selectors:
    #  - namespace: karpenter
    EOF
    
    ```
    

**í´ëŸ¬ìŠ¤í„° ìƒì„± í™•ì¸**

```bash
# í´ëŸ¬ìŠ¤í„° ìƒì„± ì™„ë£Œ, eks ë°°í¬ í™•ì¸
$eksctl get cluster
NAME	REGION		EKSCTL CREATED
myeks2	ap-northeast-2	True

$eksctl get nodegroup --cluster $CLUSTER_NAME
CLUSTER	NODEGROUP	STATUS	CREATED			MIN SIZE	MAX SIZE	DESIRED CAPACITY	INSTANCE TYPE	IMAGE ID	ASG NAME						TYPE
myeks2	myeks2-ng	ACTIVE	2023-05-22T10:04:38Z	1		10		2			m5.large	AL2_x86_64	eks-myeks2-ng-56c42175-7bbd-1c3a-4e92-b89992023b8c	managed

$eksctl get iamidentitymapping --cluster $CLUSTER_NAME
ARN												USERNAME			GROUPS					ACCOUNT
arn:aws:iam::871103481195:role/KarpenterNodeRole-myeks2						system:node:{{EC2PrivateDNSName}}	system:bootstrappers,system:nodes
arn:aws:iam::871103481195:role/eksctl-myeks2-nodegroup-myeks2-ng-NodeInstanceRole-1B2P33I00KCYF	system:node:{{EC2PrivateDNSName}}	system:bootstrappers,system:nodes

$eksctl get iamserviceaccount --cluster $CLUSTER_NAME
NAMESPACE	NAME		ROLE ARN
karpenter	karpenter	arn:aws:iam::871103481195:role/myeks2-karpenter
kube-system	aws-node	arn:aws:iam::871103481195:role/eksctl-myeks2-addon-iamserviceaccount-kube-s-Role1-18H2GYOABHE8X

# $eksctl get addon --cluster $CLUSTER_NAME
# 2023-05-22 20:41:43 [â„¹]  Kubernetes version "1.24" in use by cluster "myeks2"
# 2023-05-22 20:41:43 [â„¹]  getting all addons
# No addons found

# kubernetes cluster ì •ë³´ í™•ì¸
$kubectl cluster-info
Kubernetes control plane is running at https://AC31499751431C8E285778113EC3F0B3.gr7.ap-northeast-2.eks.amazonaws.com
CoreDNS is running at https://AC31499751431C8E285778113EC3F0B3.gr7.ap-northeast-2.eks.amazonaws.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

$kubectl get node --label-columns=node.kubernetes.io/instance-type,eks.amazonaws.com/capacityType,topology.kubernetes.io/zone
NAME                                                STATUS   ROLES    AGE   VERSION                INSTANCE-TYPE   CAPACITYTYPE   ZONE
ip-192-168-30-154.ap-northeast-2.compute.internal   Ready    <none>   96m   v1.24.13-eks-0a21954   m5.large        ON_DEMAND      ap-northeast-2a
ip-192-168-86-220.ap-northeast-2.compute.internal   Ready    <none>   96m   v1.24.13-eks-0a21954   m5.large        ON_DEMAND      ap-northeast-2c

$kubectl get pod -n kube-system -owide
NAME                      READY   STATUS    RESTARTS   AGE    IP               NODE                                                NOMINATED NODE   READINESS GATES
aws-node-9fh2n            1/1     Running   0          96m    192.168.86.220   ip-192-168-86-220.ap-northeast-2.compute.internal   <none>           <none>
aws-node-tk9mc            1/1     Running   0          96m    192.168.30.154   ip-192-168-30-154.ap-northeast-2.compute.internal   <none>           <none>
coredns-dc4979556-98j9g   1/1     Running   0          104m   192.168.26.223   ip-192-168-30-154.ap-northeast-2.compute.internal   <none>           <none>
coredns-dc4979556-lw7gf   1/1     Running   0          104m   192.168.6.105    ip-192-168-30-154.ap-northeast-2.compute.internal   <none>           <none>
kube-proxy-gzs29          1/1     Running   0          96m    192.168.86.220   ip-192-168-86-220.ap-northeast-2.compute.internal   <none>           <none>
kube-proxy-hlnpt          1/1     Running   0          96m    192.168.30.154   ip-192-168-30-154.ap-northeast-2.compute.internal   <none>           <none>

$kubectl describe cm -n kube-system aws-auth
Name:         aws-auth
Namespace:    kube-system
Labels:       <none>
Annotations:  <none>

Data
====
mapRoles:
----
- groups:
  - system:bootstrappers
  - system:nodes
  ## ì£¼ìš”
  rolearn: arn:aws:iam::871103481195:role/KarpenterNodeRole-myeks2
  username: system:node:{{EC2PrivateDNSName}}
- groups:
  - system:bootstrappers
  - system:nodes
  ## ì£¼ìš”
  rolearn: arn:aws:iam::871103481195:role/eksctl-myeks2-nodegroup-myeks2-ng-NodeInstanceRole-1B2P33I00KCYF
  username: system:node:{{EC2PrivateDNSName}}

mapUsers:
----
[]

BinaryData
====

Events:  <none>

```

 ì¹´íœí„° ì„¤ì¹˜!

```bash
# ì„¤ì • ë³€ìˆ˜ ëŒ€ì…
$export CLUSTER_ENDPOINT="$(aws eks describe-cluster --name ${CLUSTER_NAME} --query "cluster.endpoint" --output text)"
$export KARPENTER_IAM_ROLE_ARN="arn:aws:iam::${AWS_ACCOUNT_ID}:role/${CLUSTER_NAME}-karpenter"
$echo $CLUSTER_ENDPOINT $KARPENTER_IAM_ROLE_ARN
https://AC31499751431C8E285778113EC3F0B3.gr7.ap-northeast-2.eks.amazonaws.com arn:aws:iam::871103481195:role/myeks2-karpenter

# IAM ìƒì„±
$aws iam create-service-linked-role --aws-service-name spot.amazonaws.com || true
{
    "Role": {
        "Path": "/aws-service-role/spot.amazonaws.com/",
        "RoleName": "AWSServiceRoleForEC2Spot",
        "RoleId": "AROA4VUOQIVVYLQHDALB7",
        "Arn": "arn:aws:iam::871103481195:role/aws-service-role/spot.amazonaws.com/AWSServiceRoleForEC2Spot",
        "CreateDate": "2023-05-22T11:42:43+00:00",
        "AssumeRolePolicyDocument": {
            "Version": "2012-10-17",
            "Statement": [
                {
                    "Action": [
                        "sts:AssumeRole"
                    ],
                    "Effect": "Allow",
                    "Principal": {
                        "Service": [
                            "spot.amazonaws.com"
                        ]
                    }
                }
            ]
        }
    }
}

# docker logout : Logout of docker to perform an unauthenticated pull against the public ECR
$docker logout public.ecr.aws
Removing login credentials for public.ecr.aws

# karpenter ì„¤ì¹˜
$helm upgrade --install karpenter oci://public.ecr.aws/karpenter/karpenter --version ${KARPENTER_VERSION} --namespace karpenter --create-namespace \
>   --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"=${KARPENTER_IAM_ROLE_ARN} \
>   --set settings.aws.clusterName=${CLUSTER_NAME} \
>   --set settings.aws.defaultInstanceProfile=KarpenterNodeInstanceProfile-${CLUSTER_NAME} \
>   --set settings.aws.interruptionQueueName=${CLUSTER_NAME} \
>   --set controller.resources.requests.cpu=1 \
>   --set controller.resources.requests.memory=1Gi \
>   --set controller.resources.limits.cpu=1 \
>   --set controller.resources.limits.memory=1Gi \
>   --wait
Release "karpenter" does not exist. Installing it now.
Pulled: public.ecr.aws/karpenter/karpenter:v0.27.5
Digest: sha256:9491ba645592ab9485ca8ce13f53193826044522981d75975897d229b877d4c2
NAME: karpenter
LAST DEPLOYED: Mon May 22 20:42:54 2023
NAMESPACE: karpenter
STATUS: deployed
REVISION: 1
TEST SUITE: None

# karpenter ì„¤ì¹˜ í™•ì¸
$kubectl get-all -n karpenter
NAME                                                 NAMESPACE  AGE
configmap/config-logging                             karpenter  17s
configmap/karpenter-global-settings                  karpenter  17s
configmap/kube-root-ca.crt                           karpenter  17s
endpoints/karpenter                                  karpenter  17s
pod/karpenter-6c6bdb7766-2kq5b                       karpenter  16s
pod/karpenter-6c6bdb7766-bj6nn                       karpenter  16s
secret/karpenter-cert                                karpenter  17s
secret/sh.helm.release.v1.karpenter.v1               karpenter  17s
serviceaccount/default                               karpenter  17s
serviceaccount/karpenter                             karpenter  17s
service/karpenter                                    karpenter  17s
deployment.apps/karpenter                            karpenter  17s
replicaset.apps/karpenter-6c6bdb7766                 karpenter  17s
lease.coordination.k8s.io/karpenter-leader-election  karpenter  8s
endpointslice.discovery.k8s.io/karpenter-mt5ph       karpenter  17s
poddisruptionbudget.policy/karpenter                 karpenter  17s
rolebinding.rbac.authorization.k8s.io/karpenter      karpenter  17s
role.rbac.authorization.k8s.io/karpenter             karpenter  17s

$kubectl get cm -n karpenter karpenter-global-settings -o jsonpath={.data} | jq
{
  "aws.clusterEndpoint": "",
  "aws.clusterName": "myeks2",
  "aws.defaultInstanceProfile": "KarpenterNodeInstanceProfile-myeks2",
  "aws.enableENILimitedPodDensity": "true",
  "aws.enablePodENI": "false",
  "aws.interruptionQueueName": "myeks2",
  "aws.isolatedVPC": "false",
  "aws.nodeNameConvention": "ip-name",
  "aws.vmMemoryOverheadPercent": "0.075",
  "batchIdleDuration": "1s",
  "batchMaxDuration": "10s",
  "featureGates.driftEnabled": "false"
}

$kubectl get crd | grep karpenter
awsnodetemplates.karpenter.k8s.aws           2023-05-22T11:42:54Z
provisioners.karpenter.sh                    2023-05-22T11:42:54Z

```

>
**Create Provisioner : ê´€ë¦¬ ë¦¬ì†ŒìŠ¤ëŠ” securityGroupSelector and subnetSelector ë¡œ ì°¾ìŒ, ttlSecondsAfterEmpty(ë¯¸ì‚¬ìš© ë…¸ë“œ ì •ë¦¬, ë°ëª¬ì…‹ ì œì™¸)**
>

**provisioner install**

```bash
$cat <<EOF | kubectl apply -f -
> apiVersion: karpenter.sh/v1alpha5
> kind: Provisioner
> metadata:
>   name: default
> spec:
>   requirements:
>     - key: karpenter.sh/capacity-type
>       operator: In
>       values: ["spot"]
>   limits:
>     resources:
>       cpu: 1000
>   providerRef:
>     name: default
>   ttlSecondsAfterEmpty: 30
> ---
> apiVersion: karpenter.k8s.aws/v1alpha1
> kind: AWSNodeTemplate
> metadata:
>   name: default
> spec:
>   subnetSelector:
>     karpenter.sh/discovery: ${CLUSTER_NAME}
>   securityGroupSelector:
>     karpenter.sh/discovery: ${CLUSTER_NAME}
> EOF
provisioner.karpenter.sh/default created
awsnodetemplate.karpenter.k8s.aws/default created

```

**provisioners ì„¤ì¹˜ í™•ì¸**

```bash
$kubectl get awsnodetemplates,provisioners
NAME                                        AGE
awsnodetemplate.karpenter.k8s.aws/default   9s

NAME                               AGE
provisioner.karpenter.sh/default   9s

```

**external dns, kube-ops-view, ê·¸ë¼íŒŒë‚˜, í”„ë¡œë©”í…Œìš°ìŠ¤ ì„¤ì¹˜!**

```bash
$helm repo add grafana-charts https://grafana.github.io/helm-charts
"grafana-charts" has been added to your repositories
$helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
"prometheus-community" has been added to your repositories
$helm repo update
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "grafana-charts" chart repository
...Successfully got an update from the "prometheus-community" chart repository
Update Complete. âˆHappy Helming!âˆ
$kubectl create namespace monitoring
namespace/monitoring created
$curl -fsSL https://karpenter.sh/"${KARPENTER_VERSION}"/getting-started/getting-started-with-karpenter/prometheus-values.yaml | tee prometheus-values.yaml
alertmanager:
  persistentVolume:
    enabled: false

server:
  fullnameOverride: prometheus-server
  persistentVolume:
    enabled: false

extraScrapeConfigs: |
    - job_name: karpenter
      kubernetes_sd_configs:
      - role: endpoints
        namespaces:
          names:
          - karpenter
      relabel_configs:
      - source_labels: [__meta_kubernetes_endpoint_port_name]
        regex: http-metrics
        action: keep

$helm install --namespace monitoring prometheus prometheus-community/prometheus --values prometheus-values.yaml --set alertmanager.enabled=false

NAME: prometheus
LAST DEPLOYED: Mon May 22 20:44:27 2023
NAMESPACE: monitoring
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
The Prometheus server can be accessed via port 80 on the following DNS name from within your cluster:
prometheus-server.monitoring.svc.cluster.local

Get the Prometheus server URL by running these commands in the same shell:
  export POD_NAME=$(kubectl get pods --namespace monitoring -l "app=prometheus,component=server" -o jsonpath="{.items[0].metadata.name}")
  kubectl --namespace monitoring port-forward $POD_NAME 9090
#################################################################################
######   WARNING: Persistence is disabled!!! You will lose your data when   #####
######            the Server pod is terminated.                             #####
#################################################################################

#################################################################################
######   WARNING: Pod Security Policy has been disabled by default since    #####
######            it deprecated after k8s 1.25+. use                        #####
######            (index .Values "prometheus-node-exporter" "rbac"          #####
###### .          "pspEnabled") with (index .Values                         #####
######            "prometheus-node-exporter" "rbac" "pspAnnotations")       #####
######            in case you still need it.                                #####
#################################################################################

The Prometheus PushGateway can be accessed via port 9091 on the following DNS name from within your cluster:
prometheus-prometheus-pushgateway.monitoring.svc.cluster.local

Get the PushGateway URL by running these commands in the same shell:
  export POD_NAME=$(kubectl get pods --namespace monitoring -l "app=prometheus-pushgateway,component=pushgateway" -o jsonpath="{.items[0].metadata.name}")
  kubectl --namespace monitoring port-forward $POD_NAME 9091

For more information on running Prometheus, visit:
https://prometheus.io/

$curl -fsSL https://karpenter.sh/"${KARPENTER_VERSION}"/getting-started/getting-started-with-karpenter/grafana-values.yaml | tee grafana-values.yaml
datasources:
  datasources.yaml:
    apiVersion: 1
    datasources:
    - name: Prometheus
      type: prometheus
      version: 1
      url: http://prometheus-server:80
      access: proxy
dashboardProviders:
  dashboardproviders.yaml:
    apiVersion: 1
    providers:
    - name: 'default'
      orgId: 1
      folder: ''
      type: file
      disableDeletion: false
      editable: true
      options:
        path: /var/lib/grafana/dashboards/default
dashboards:
  default:
    capacity-dashboard:
      url: https://karpenter.sh/v0.27.5/getting-started/getting-started-with-karpenter/karpenter-capacity-dashboard.json
    performance-dashboard:
      url: https://karpenter.sh/v0.27.5/getting-started/getting-started-with-karpenter/karpenter-performance-dashboard.json
$helm install --namespace monitoring grafana grafana-charts/grafana --values grafana-values.yaml --set service.type=LoadBalancer
NAME: grafana
LAST DEPLOYED: Mon May 22 20:44:35 2023
NAMESPACE: monitoring
STATUS: deployed
REVISION: 1
NOTES:
1. Get your 'admin' user password by running:

   kubectl get secret --namespace monitoring grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo

2. The Grafana server can be accessed via port 80 on the following DNS name from within your cluster:

   grafana.monitoring.svc.cluster.local

   Get the Grafana URL to visit by running these commands in the same shell:
   NOTE: It may take a few minutes for the LoadBalancer IP to be available.
        You can watch the status of by running 'kubectl get svc --namespace monitoring -w grafana'
     export SERVICE_IP=$(kubectl get svc --namespace monitoring grafana -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
     http://$SERVICE_IP:80

3. Login with the password from step 1 and the username: admin
#################################################################################
######   WARNING: Persistence is disabled!!! You will lose your data when   #####
######            the Grafana pod is terminated.                            #####
#################################################################################

$kubectl get secret --namespace monitoring grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo
rPBZBl47fe72XObwGKsXNYtvkGGEWA8FCFYXujwC
$MyDomain=kaneawsdns.com
$echo "export MyDomain=kaneawsdns.com" >> /etc/profile
$MyDnzHostedZoneId=$(aws route53 list-hosted-zones-by-name --dns-name "${MyDomain}." --query "HostedZones[0].Id" --output text)

$echo $MyDomain, $MyDnzHostedZoneId
kaneawsdns.com, /hostedzone/Z06702063E7RRITLLMJRM

$curl -s -O https://raw.githubusercontent.com/gasida/PKOS/main/aews/externaldns.yaml
$MyDomain=$MyDomain MyDnzHostedZoneId=$MyDnzHostedZoneId envsubst < externaldns.yaml | kubectl apply -f -

serviceaccount/external-dns created
clusterrole.rbac.authorization.k8s.io/external-dns created
clusterrolebinding.rbac.authorization.k8s.io/external-dns-viewer created
deployment.apps/external-dns created

$kubectl annotate service grafana -n monitoring "external-dns.alpha.kubernetes.io/hostname=grafana.$MyDomain"
service/grafana annotated

$echo -e "grafana URL = http://grafana.$MyDomain"
grafana URL = http://grafana.kaneawsdns.com

#kube-ops-view install
$helm repo add geek-cookbook https://geek-cookbook.github.io/charts/

"geek-cookbook" has been added to your repositories

$helm install kube-ops-view geek-cookbook/kube-ops-view --version 1.2.2 --set env.TZ="Asia/Seoul" --namespace kube-system

NAME: kube-ops-view
LAST DEPLOYED: Mon May 22 20:47:00 2023
NAMESPACE: kube-system
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace kube-system -l "app.kubernetes.io/name=kube-ops-view,app.kubernetes.io/instance=kube-ops-view" -o jsonpath="{.items[0].metadata.name}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl port-forward $POD_NAME 8080:8080

$kubectl patch svc -n kube-system kube-ops-view -p '{"spec":{"type":"LoadBalancer"}}'

service/kube-ops-view patched
$kubectl annotate service kube-ops-view -n kube-system "external-dns.alpha.kubernetes.io/hostname=kubeopsview.$MyDomain"
service/kube-ops-view annotated
$echo -e "Kube Ops View URL = http://kubeopsview.$MyDomain:8080/#scale=1.5"
Kube Ops View URL = http://kubeopsview.kaneawsdns.com:8080/#scale=1.5
$kubectl get awsnodetemplates,provisioners
NAME                                        AGE
awsnodetemplate.karpenter.k8s.aws/default   3m40s

NAME                               AGE
provisioner.karpenter.sh/default   3m40s

```

í…ŒìŠ¤íŠ¸ ì‹¤ì‹œ!

íŒŒë“œ í•œ ê°œë‹¹ ìš”êµ¬ëŸ‰ì„ 1ì½”ì–´ë¡œ ë‘ê³ , ì´í›„ ì¦ê°€ì‹œì¼œ ì˜¤í† ìŠ¤ì¼€ì¼ë§ í™•ì¸

```bash
# pause íŒŒë“œ 1ê°œì— CPU 1ê°œ ìµœì†Œ ë³´ì¥ í• ë‹¹
$cat <<EOF | kubectl apply -f -
> apiVersion: apps/v1
> kind: Deployment
> metadata:
>   name: inflate
> spec:
>   replicas: 0
>   selector:
>     matchLabels:
>       app: inflate
>   template:
>     metadata:
>       labels:
>         app: inflate
>     spec:
>       terminationGracePeriodSeconds: 0
>       containers:
>         - name: inflate
>           image: public.ecr.aws/eks-distro/kubernetes/pause:3.7
>           resources:
>             requests:
>               cpu: 1
> EOF
deployment.apps/inflate created

```

```bash
# íŒŒë“œ ê°œìˆ˜ ì¦ê°€!
$kubectl scale deployment inflate --replicas 5
deployment.apps/inflate scaled
$kubectl logs -f -n karpenter -l app.kubernetes.io/name=karpenter -c controller
2023-05-22T11:43:55.652Z	DEBUG	controller.deprovisioning	discovered instance types	{"commit": "698f22f-dirty", "count": 366}
2023-05-22T11:43:55.743Z	DEBUG	controller.deprovisioning	discovered offerings for instance types	{"commit": "698f22f-dirty", "zones": ["ap-northeast-2a", "ap-northeast-2c", "ap-northeast-2d"], "instance-type-count": 367, "node-template": "default"}
2023-05-22T11:48:08.748Z	INFO	controller.provisioner	found provisionable pod(s)	{"commit": "698f22f-dirty", "pods": 5}
2023-05-22T11:48:08.748Z	INFO	controller.provisioner	computed new machine(s) to fit pod(s)	{"commit": "698f22f-dirty", "machines": 1, "pods": 5}
2023-05-22T11:48:08.749Z	INFO	controller.provisioner	launching machine with 5 pods requesting {"cpu":"5125m","pods":"8"} from types c6i.2xlarge, c6i.24xlarge, r5ad.24xlarge, r6i.24xlarge, r5.24xlarge and 135 other(s)	{"commit": "698f22f-dirty", "provisioner": "default"}
2023-05-22T11:48:09.106Z	DEBUG	controller.provisioner.cloudprovider	discovered kubernetes version	{"commit": "698f22f-dirty", "provisioner": "default", "kubernetes-version": "1.24"}
2023-05-22T11:48:09.136Z	DEBUG	controller.provisioner.cloudprovider	discovered ami	{"commit": "698f22f-dirty", "provisioner": "default", "ami": "ami-0fa3b31d56b9a36b2", "query": "/aws/service/eks/optimized-ami/1.24/amazon-linux-2/recommended/image_id"}
2023-05-22T11:48:09.163Z	DEBUG	controller.provisioner.cloudprovider	discovered ami	{"commit": "698f22f-dirty", "provisioner": "default", "ami": "ami-021b63322f1c5fc23", "query": "/aws/service/eks/optimized-ami/1.24/amazon-linux-2-gpu/recommended/image_id"}
2023-05-22T11:48:09.173Z	DEBUG	controller.provisioner.cloudprovider	discovered ami	{"commit": "698f22f-dirty", "provisioner": "default", "ami": "ami-0a31a3ce85ee4a8e6", "query": "/aws/service/eks/optimized-ami/1.24/amazon-linux-2-arm64/recommended/image_id"}
2023-05-22T11:48:09.311Z	DEBUG	controller.provisioner.cloudprovider	created launch template	{"commit": "698f22f-dirty", "provisioner": "default", "launch-template-name": "karpenter.k8s.aws/16624063517551845275", "launch-template-id": "lt-0458277a0a9530173"}
2023-05-22T11:43:03.998Z	DEBUG	controller	discovered kube dns	{"commit": "698f22f-dirty", "kube-dns-ip": "10.100.0.10"}
2023-05-22T11:43:03.999Z	DEBUG	controller	discovered version	{"commit": "698f22f-dirty", "version": "v0.27.5"}
2023/05/22 11:43:03 Registering 2 clients
2023/05/22 11:43:03 Registering 2 informer factories
2023/05/22 11:43:03 Registering 3 informers
2023/05/22 11:43:03 Registering 5 controllers
2023-05-22T11:43:04.000Z	INFO	controller	Starting server	{"commit": "698f22f-dirty", "path": "/metrics", "kind": "metrics", "addr": "[::]:8080"}
2023-05-22T11:43:04.000Z	INFO	controller	Starting server	{"commit": "698f22f-dirty", "kind": "health probe", "addr": "[::]:8081"}
I0522 11:43:04.101075       1 leaderelection.go:248] attempting to acquire leader lease karpenter/karpenter-leader-election...
2023-05-22T11:43:04.189Z	INFO	controller	Starting informers...	{"commit": "698f22f-dirty"}
2023-05-22T11:48:11.910Z	INFO	controller.provisioner.cloudprovider	launched instance	{"commit": "698f22f-dirty", "provisioner": "default", "id": "i-0f03bf23055b38747", "hostname": "ip-192-168-188-148.ap-northeast-2.compute.internal", "instance-type": "c4.2xlarge", "zone": "ap-northeast-2c", "capacity-type": "spot", "capacity": {"cpu":"8","ephemeral-storage":"20Gi","memory":"14208Mi","pods":"58"}}

$kubectl get node --label-columns=eks.amazonaws.com/capacityType,karpenter.sh/capacity-type,node.kubernetes.io/instance-type
NAME                                                 STATUS     ROLES    AGE    VERSION                CAPACITYTYPE   CAPACITY-TYPE   INSTANCE-TYPE
ip-192-168-188-148.ap-northeast-2.compute.internal   NotReady   <none>   26s    v1.24.13-eks-0a21954                  spot            c4.2xlarge
ip-192-168-30-154.ap-northeast-2.compute.internal    Ready      <none>   102m   v1.24.13-eks-0a21954   ON_DEMAND                      m5.large
ip-192-168-86-220.ap-northeast-2.compute.internal    Ready      <none>   102m   v1.24.13-eks-0a21954   ON_DEMAND                      m5.large

$k get po -A
NAMESPACE     NAME                                                READY   STATUS              RESTARTS   AGE
default       inflate-ccf449f59-cswnb                             0/1     ContainerCreating   0          41s
default       inflate-ccf449f59-m5cq6                             0/1     ContainerCreating   0          41s
default       inflate-ccf449f59-t5fxt                             0/1     ContainerCreating   0          41s
default       inflate-ccf449f59-tl79k                             0/1     ContainerCreating   0          41s
default       inflate-ccf449f59-wtk4l                             0/1     ContainerCreating   0          41s
karpenter     karpenter-6c6bdb7766-2kq5b                          1/1     Running             0          5m52s
karpenter     karpenter-6c6bdb7766-bj6nn                          1/1     Running             0          5m52s
kube-system   aws-node-9fh2n                                      1/1     Running             0          102m
kube-system   aws-node-dmzdv                                      0/1     Running             0          37s
kube-system   aws-node-tk9mc                                      1/1     Running             0          102m
kube-system   coredns-dc4979556-98j9g                             1/1     Running             0          111m
kube-system   coredns-dc4979556-lw7gf                             1/1     Running             0          111m
kube-system   external-dns-cc5c8cd74-v4frr                        1/1     Running             0          2m55s
kube-system   kube-ops-view-558d87b798-z4wng                      1/1     Running             0          107s
kube-system   kube-proxy-gzs29                                    1/1     Running             0          102m
kube-system   kube-proxy-hlnpt                                    1/1     Running             0          102m
kube-system   kube-proxy-xb2hs                                    1/1     Running             0          37s
monitoring    grafana-b488f8cdb-8mwn5                             1/1     Running             0          4m11s
monitoring    prometheus-kube-state-metrics-6fcf5978bf-rzbh8      1/1     Running             0          4m20s
monitoring    prometheus-prometheus-node-exporter-jpfgq           1/1     Running             0          36s
monitoring    prometheus-prometheus-node-exporter-r8vqh           1/1     Running             0          4m20s
monitoring    prometheus-prometheus-node-exporter-vwsk9           1/1     Running             0          4m19s
monitoring    prometheus-prometheus-pushgateway-fdb75d75f-jdbhr   1/1     Running             0          4m20s
monitoring    prometheus-server-6f974fdfd-l7rv7                   2/2     Running             0          4m20s
```

AWS ìì› í™•ì¸

- AWS ìì› í™•ì¸
    
    ```bash
    $aws ec2 describe-spot-instance-requests --filters "Name=state,Values=active" --output table
    
    ------------------------------------------------------------------------------------------------------
    |                                    DescribeSpotInstanceRequests                                    |
    +----------------------------------------------------------------------------------------------------+
    ||                                       SpotInstanceRequests                                       ||
    |+--------------------------------------------------+-----------------------------------------------+|
    ||  CreateTime                                      |  2023-05-22T11:48:11+00:00                    ||
    ||  InstanceId                                      |  i-0f03bf23055b38747                          ||
    ||  InstanceInterruptionBehavior                    |  terminate                                    ||
    ||  LaunchedAvailabilityZone                        |  ap-northeast-2c                              ||
    ||  ProductDescription                              |  Linux/UNIX                                   ||
    ||  SpotInstanceRequestId                           |  sir-q2r6k2vp                                 ||
    ||  SpotPrice                                       |  0.454000                                     ||
    ||  State                                           |  active                                       ||
    ||  Type                                            |  one-time                                     ||
    |+--------------------------------------------------+-----------------------------------------------+|
    |||                                       LaunchSpecification                                      |||
    ||+------------------------------------+-----------------------------------------------------------+||
    |||  ImageId                           |  ami-0fa3b31d56b9a36b2                                    |||
    |||  InstanceType                      |  c4.2xlarge                                               |||
    ||+------------------------------------+-----------------------------------------------------------+||
    ||||                                      BlockDeviceMappings                                     ||||
    |||+------------------------------------------------+---------------------------------------------+|||
    ||||  DeviceName                                    |  /dev/xvda                                  ||||
    |||+------------------------------------------------+---------------------------------------------+|||
    |||||                                             Ebs                                            |||||
    ||||+--------------------------------------------------------------------+-----------------------+||||
    |||||  DeleteOnTermination                                               |  True                 |||||
    |||||  Encrypted                                                         |  True                 |||||
    |||||  VolumeSize                                                        |  20                   |||||
    |||||  VolumeType                                                        |  gp3                  |||||
    ||||+--------------------------------------------------------------------+-----------------------+||||
    ||||                                      IamInstanceProfile                                      ||||
    |||+-------+--------------------------------------------------------------------------------------+|||
    ||||  Arn  |  arn:aws:iam::871103481195:instance-profile/KarpenterNodeInstanceProfile-myeks2      ||||
    ||||  Name |  KarpenterNodeInstanceProfile-myeks2                                                 ||||
    |||+-------+--------------------------------------------------------------------------------------+|||
    ||||                                          Monitoring                                          ||||
    |||+---------------------------------------------------+------------------------------------------+|||
    ||||  Enabled                                          |  False                                   ||||
    |||+---------------------------------------------------+------------------------------------------+|||
    ||||                                       NetworkInterfaces                                      ||||
    |||+-----------------------------------------+----------------------------------------------------+|||
    ||||  DeleteOnTermination                    |  True                                              ||||
    ||||  DeviceIndex                            |  0                                                 ||||
    ||||  SubnetId                               |  subnet-04e3bff3afc9245a6                          ||||
    |||+-----------------------------------------+----------------------------------------------------+|||
    ||||                                           Placement                                          ||||
    |||+-----------------------------------------------+----------------------------------------------+|||
    ||||  AvailabilityZone                             |  ap-northeast-2c                             ||||
    ||||  Tenancy                                      |  default                                     ||||
    |||+-----------------------------------------------+----------------------------------------------+|||
    ||||                                        SecurityGroups                                        ||||
    |||+-----------------------+----------------------------------------------------------------------+|||
    ||||        GroupId        |                              GroupName                               ||||
    |||+-----------------------+----------------------------------------------------------------------+|||
    ||||  sg-0ba1ccd4016a58521 |  eksctl-myeks2-cluster-ControlPlaneSecurityGroup-WNSB7WW4HLIF        ||||
    ||||  sg-0317cb7e7df34881a |  eksctl-myeks2-cluster-ClusterSharedNodeSecurityGroup-1VRI6GJ40ZT7A  ||||
    |||+-----------------------+----------------------------------------------------------------------+|||
    |||                                             Status                                             |||
    ||+--------------------------+---------------------------------------------------------------------+||
    |||  Code                    |  fulfilled                                                          |||
    |||  Message                 |  Your Spot request is fulfilled.                                    |||
    |||  UpdateTime              |  2023-05-22T11:48:22+00:00                                          |||
    ||+--------------------------+---------------------------------------------------------------------+||
    
    $kubectl get node -l karpenter.sh/capacity-type=spot -o jsonpath='{.items[0].metadata.labels}' | jq
    {
      "beta.kubernetes.io/arch": "amd64",
      "beta.kubernetes.io/os": "linux",
      "k8s.io/cloud-provider-aws": "35a3405a9b5c02025fe6ff647a94190b",
      "karpenter.k8s.aws/instance-ami-id": "ami-0fa3b31d56b9a36b2",
      "karpenter.k8s.aws/instance-category": "c",
      "karpenter.k8s.aws/instance-cpu": "8",
      "karpenter.k8s.aws/instance-encryption-in-transit-supported": "false",
      "karpenter.k8s.aws/instance-family": "c4",
      "karpenter.k8s.aws/instance-generation": "4",
      "karpenter.k8s.aws/instance-hypervisor": "xen",
      "karpenter.k8s.aws/instance-memory": "15360",
      "karpenter.k8s.aws/instance-network-bandwidth": "2500",
      "karpenter.k8s.aws/instance-pods": "58",
      "karpenter.k8s.aws/instance-size": "2xlarge",
      "karpenter.sh/capacity-type": "spot",
      "karpenter.sh/provisioner-name": "default",
      "kubernetes.io/arch": "amd64",
      "kubernetes.io/os": "linux",
      "node.kubernetes.io/instance-type": "c4.2xlarge",
      "topology.kubernetes.io/region": "ap-northeast-2",
      "topology.kubernetes.io/zone": "ap-northeast-2c"
    }
    
    ```
    

í…ŒìŠ¤íŠ¸ ì¢…ë£Œ

```bash
$kubectl delete deployment inflate
deployment.apps "inflate" deleted

$kubectl logs -f -n karpenter -l app.kubernetes.io/name=karpenter -c controller
2023-05-22T11:43:03.998Z	DEBUG	controller	discovered kube dns	{"commit": "698f22f-dirty", "kube-dns-ip": "10.100.0.10"}
2023-05-22T11:43:03.999Z	DEBUG	controller	discovered version	{"commit": "698f22f-dirty", "version": "v0.27.5"}
2023/05/22 11:43:03 Registering 2 clients
2023/05/22 11:43:03 Registering 2 informer factories
2023/05/22 11:43:03 Registering 3 informers
2023/05/22 11:43:03 Registering 5 controllers
2023-05-22T11:43:04.000Z	INFO	controller	Starting server	{"commit": "698f22f-dirty", "path": "/metrics", "kind": "metrics", "addr": "[::]:8080"}
2023-05-22T11:43:04.000Z	INFO	controller	Starting server	{"commit": "698f22f-dirty", "kind": "health probe", "addr": "[::]:8081"}
I0522 11:43:04.101075       1 leaderelection.go:248] attempting to acquire leader lease karpenter/karpenter-leader-election...
2023-05-22T11:43:04.189Z	INFO	controller	Starting informers...	{"commit": "698f22f-dirty"}
2023-05-22T11:48:09.106Z	DEBUG	controller.provisioner.cloudprovider	discovered kubernetes version	{"commit": "698f22f-dirty", "provisioner": "default", "kubernetes-version": "1.24"}
2023-05-22T11:48:09.136Z	DEBUG	controller.provisioner.cloudprovider	discovered ami	{"commit": "698f22f-dirty", "provisioner": "default", "ami": "ami-0fa3b31d56b9a36b2", "query": "/aws/service/eks/optimized-ami/1.24/amazon-linux-2/recommended/image_id"}
2023-05-22T11:48:09.163Z	DEBUG	controller.provisioner.cloudprovider	discovered ami	{"commit": "698f22f-dirty", "provisioner": "default", "ami": "ami-021b63322f1c5fc23", "query": "/aws/service/eks/optimized-ami/1.24/amazon-linux-2-gpu/recommended/image_id"}
2023-05-22T11:48:09.173Z	DEBUG	controller.provisioner.cloudprovider	discovered ami	{"commit": "698f22f-dirty", "provisioner": "default", "ami": "ami-0a31a3ce85ee4a8e6", "query": "/aws/service/eks/optimized-ami/1.24/amazon-linux-2-arm64/recommended/image_id"}
2023-05-22T11:48:09.311Z	DEBUG	controller.provisioner.cloudprovider	created launch template	{"commit": "698f22f-dirty", "provisioner": "default", "launch-template-name": "karpenter.k8s.aws/16624063517551845275", "launch-template-id": "lt-0458277a0a9530173"}
2023-05-22T11:48:11.910Z	INFO	controller.provisioner.cloudprovider	launched instance	{"commit": "698f22f-dirty", "provisioner": "default", "id": "i-0f03bf23055b38747", "hostname": "ip-192-168-188-148.ap-northeast-2.compute.internal", "instance-type": "c4.2xlarge", "zone": "ap-northeast-2c", "capacity-type": "spot", "capacity": {"cpu":"8","ephemeral-storage":"20Gi","memory":"14208Mi","pods":"58"}}
2023-05-22T11:50:24.721Z	DEBUG	controller.node	added TTL to empty node	{"commit": "698f22f-dirty", "node": "ip-192-168-188-148.ap-northeast-2.compute.internal"}
2023-05-22T11:50:56.648Z	INFO	controller.deprovisioning	deprovisioning via emptiness delete, terminating 1 machines ip-192-168-188-148.ap-northeast-2.compute.internal/c4.2xlarge/spot	{"commit": "698f22f-dirty"}
2023-05-22T11:50:56.682Z	INFO	controller.termination	cordoned node	{"commit": "698f22f-dirty", "node": "ip-192-168-188-148.ap-northeast-2.compute.internal"}
2023-05-22T11:50:57.064Z	INFO	controller.termination	deleted node	{"commit": "698f22f-dirty", "node": "ip-192-168-188-148.ap-northeast-2.compute.internal"}

$kubectl delete provisioners default
provisioner.karpenter.sh "default" deleted

```

ìƒˆë¡œìš´ Provisiner ì„¤ì¹˜

```bash

$cat <<EOF | kubectl apply -f -
> apiVersion: karpenter.sh/v1alpha5
> kind: Provisioner
> metadata:
>   name: default
> spec:
>   consolidation:
>     enabled: true
>   labels:
>     type: karpenter
>   limits:
>     resources:
>       cpu: 1000
>       memory: 1000Gi
>   providerRef:
>     name: default
>   requirements:
>     - key: karpenter.sh/capacity-type
>       operator: In
>       values:
>         - on-demand
**>     - key: node.kubernetes.io/instance-type
>       operator: In
>       values:
>         - c5.large
>         - m5.large
>         - m5.xlarge**
> EOF
provisioner.karpenter.sh/default created

```

í…ŒìŠ¤íŠ¸ìš© ë””í”Œë¡œì´ë¨¼íŠ¸ ë°°í¬

```bash
$cat <<EOF | kubectl apply -f -
> apiVersion: apps/v1
> kind: Deployment
> metadata:
>   name: inflate
> spec:
>   replicas: 0
>   selector:
>     matchLabels:
>       app: inflate
>   template:
>     metadata:
>       labels:
>         app: inflate
>     spec:
>       terminationGracePeriodSeconds: 0
>       containers:
>         - name: inflate
>           image: public.ecr.aws/eks-distro/kubernetes/pause:3.7
>           resources:
>             requests:
>               cpu: 1
> EOF
deployment.apps/inflate created

```

ì˜¤í† ìŠ¤ì¼€ì¼ë§ í™•ì¸ â†’ íŒŒë“œê°œìˆ˜ ì¦ê°€ì‹œí‚¤ê¸°

```bash
$kubectl scale deployment inflate --replicas 12
deployment.apps/inflate scaled
```

ë¡œê·¸ë¡œ ì‹œê°„ í™•ì¸ â†’ 10ì´ˆ ì´ë‚´ë¡œ ë°˜ì‘

```bash

$kubectl logs -f -n karpenter -l app.kubernetes.io/name=karpenter -c controller
2023-05-22T11:43:03.998Z	DEBUG	controller	discovered kube dns	{"commit": "698f22f-dirty", "kube-dns-ip": "10.100.0.10"}
2023-05-22T11:43:03.999Z	DEBUG	controller	discovered version	{"commit": "698f22f-dirty", "version": "v0.27.5"}
2023/05/22 11:43:03 Registering 2 clients
2023/05/22 11:43:03 Registering 2 informer factories
2023/05/22 11:43:03 Registering 3 informers
2023/05/22 11:43:03 Registering 5 controllers
2023-05-22T11:43:04.000Z	INFO	controller	Starting server	{"commit": "698f22f-dirty", "path": "/metrics", "kind": "metrics", "addr": "[::]:8080"}
2023-05-22T11:43:04.000Z	INFO	controller	Starting server	{"commit": "698f22f-dirty", "kind": "health probe", "addr": "[::]:8081"}
I0522 11:43:04.101075       1 leaderelection.go:248] attempting to acquire leader lease karpenter/karpenter-leader-election...
2023-05-22T11:43:04.189Z	INFO	controller	Starting informers...	{"commit": "698f22f-dirty"}
2023-05-22T11:52:47.012Z	INFO	controller.provisioner	computed new machine(s) to fit pod(s)	{"commit": "698f22f-dirty", "machines": 4, "pods": 12}
2023-05-22T11:52:47.012Z	INFO	controller.provisioner	launching machine with 3 pods requesting {"cpu":"3125m","pods":"6"} from types m5.xlarge	{"commit": "698f22f-dirty", "provisioner": "default"}
2023-05-22T11:52:47.015Z	INFO	controller.provisioner	launching machine with 3 pods requesting {"cpu":"3125m","pods":"6"} from types m5.xlarge	{"commit": "698f22f-dirty", "provisioner": "default"}
2023-05-22T11:52:47.021Z	INFO	controller.provisioner	launching machine with 3 pods requesting {"cpu":"3125m","pods":"6"} from types m5.xlarge	{"commit": "698f22f-dirty", "provisioner": "default"}
2023-05-22T11:52:47.026Z	INFO	controller.provisioner	launching machine with 3 pods requesting {"cpu":"3125m","pods":"6"} from types m5.xlarge	{"commit": "698f22f-dirty", "provisioner": "default"}
2023-05-22T11:52:47.562Z	DEBUG	controller.provisioner.cloudprovider	created launch template	{"commit": "698f22f-dirty", "provisioner": "default", "launch-template-name": "karpenter.k8s.aws/10691513453991989385", "launch-template-id": "lt-0bdf1b82218c3ff67"}
2023-05-22T11:52:49.588Z	INFO	controller.provisioner.cloudprovider	launched instance	{"commit": "698f22f-dirty", "provisioner": "default", "id": "i-03691dff7f7c5089b", "hostname": "ip-192-168-95-67.ap-northeast-2.compute.internal", "instance-type": "m5.xlarge", "zone": "ap-northeast-2c", "capacity-type": "on-demand", "capacity": {"cpu":"4","ephemeral-storage":"20Gi","memory":"15155Mi","pods":"58"}}
2023-05-22T11:52:49.588Z	INFO	controller.provisioner.cloudprovider	launched instance	{"commit": "698f22f-dirty", "provisioner": "default", "id": "i-0dd5ad429b8a4fb1c", "hostname": "ip-192-168-70-7.ap-northeast-2.compute.internal", "instance-type": "m5.xlarge", "zone": "ap-northeast-2c", "capacity-type": "on-demand", "capacity": {"cpu":"4","ephemeral-storage":"20Gi","memory":"15155Mi","pods":"58"}}
2023-05-22T11:52:49.588Z	INFO	controller.provisioner.cloudprovider	launched instance	{"commit": "698f22f-dirty", "provisioner": "default", "id": "i-0c0613125e339e499", "hostname": "ip-192-168-182-17.ap-northeast-2.compute.internal", "instance-type": "m5.xlarge", "zone": "ap-northeast-2c", "capacity-type": "on-demand", "capacity": {"cpu":"4","ephemeral-storage":"20Gi","memory":"15155Mi","pods":"58"}}
2023-05-22T11:52:49.588Z	INFO	controller.provisioner.cloudprovider	launched instance	{"commit": "698f22f-dirty", "provisioner": "default", "id": "i-08a1e375dc91770ff", "hostname": "ip-192-168-174-179.ap-northeast-2.compute.internal", "instance-type": "m5.xlarge", "zone": "ap-northeast-2c", "capacity-type": "on-demand", "capacity": {"cpu":"4","ephemeral-storage":"20Gi","memory":"15155Mi","pods":"58"}}

$kubectl get node --label-columns=node.kubernetes.io/instance-type,topology.kubernetes.io/zone
NAME                                                 STATUS   ROLES    AGE    VERSION                INSTANCE-TYPE   ZONE
ip-192-168-174-179.ap-northeast-2.compute.internal   Ready    <none>   45s    v1.24.13-eks-0a21954   m5.xlarge       ap-northeast-2c
ip-192-168-182-17.ap-northeast-2.compute.internal    Ready    <none>   45s    v1.24.13-eks-0a21954   m5.xlarge       ap-northeast-2c
ip-192-168-30-154.ap-northeast-2.compute.internal    Ready    <none>   107m   v1.24.13-eks-0a21954   m5.large        ap-northeast-2a
ip-192-168-70-7.ap-northeast-2.compute.internal      Ready    <none>   45s    v1.24.13-eks-0a21954   m5.xlarge       ap-northeast-2c
ip-192-168-86-220.ap-northeast-2.compute.internal    Ready    <none>   107m   v1.24.13-eks-0a21954   m5.large        ap-northeast-2c
ip-192-168-95-67.ap-northeast-2.compute.internal     Ready    <none>   45s    v1.24.13-eks-0a21954   m5.xlarge       ap-northeast-2c

```

ìŠ¤ì¼€ì¼ ë‹¤ìš´! â†’ ì˜¤í† ìŠ¤ì¼€ì¼ë§ í™•ì¸ 

```bash
$kubectl scale deployment inflate --replicas 5
deployment.apps/inflate scaled
```

ë¡œê·¸ í™•ì¸

```bash
$kubectl logs -f -n karpenter -l app.kubernetes.io/name=karpenter -c controller
2023-05-22T11:43:03.998Z	DEBUG	controller	discovered kube dns	{"commit": "698f22f-dirty", "kube-dns-ip": "10.100.0.10"}
2023-05-22T11:43:03.999Z	DEBUG	controller	discovered version	{"commit": "698f22f-dirty", "version": "v0.27.5"}
2023/05/22 11:43:03 Registering 2 clients
2023/05/22 11:43:03 Registering 2 informer factories
2023/05/22 11:43:03 Registering 3 informers
2023/05/22 11:43:03 Registering 5 controllers
2023-05-22T11:43:04.000Z	INFO	controller	Starting server	{"commit": "698f22f-dirty", "path": "/metrics", "kind": "metrics", "addr": "[::]:8080"}
2023-05-22T11:43:04.000Z	INFO	controller	Starting server	{"commit": "698f22f-dirty", "kind": "health probe", "addr": "[::]:8081"}
I0522 11:43:04.101075       1 leaderelection.go:248] attempting to acquire leader lease karpenter/karpenter-leader-election...
2023-05-22T11:43:04.189Z	INFO	controller	Starting informers...	{"commit": "698f22f-dirty"}
2023-05-22T11:52:47.012Z	INFO	controller.provisioner	launching machine with 3 pods requesting {"cpu":"3125m","pods":"6"} from types m5.xlarge	{"commit": "698f22f-dirty", "provisioner": "default"}
2023-05-22T11:52:47.015Z	INFO	controller.provisioner	launching machine with 3 pods requesting {"cpu":"3125m","pods":"6"} from types m5.xlarge	{"commit": "698f22f-dirty", "provisioner": "default"}
2023-05-22T11:52:47.021Z	INFO	controller.provisioner	launching machine with 3 pods requesting {"cpu":"3125m","pods":"6"} from types m5.xlarge	{"commit": "698f22f-dirty", "provisioner": "default"}
2023-05-22T11:52:47.026Z	INFO	controller.provisioner	launching machine with 3 pods requesting {"cpu":"3125m","pods":"6"} from types m5.xlarge	{"commit": "698f22f-dirty", "provisioner": "default"}
2023-05-22T11:52:47.562Z	DEBUG	controller.provisioner.cloudprovider	created launch template	{"commit": "698f22f-dirty", "provisioner": "default", "launch-template-name": "karpenter.k8s.aws/10691513453991989385", "launch-template-id": "lt-0bdf1b82218c3ff67"}
2023-05-22T11:52:49.588Z	INFO	controller.provisioner.cloudprovider	launched instance	{"commit": "698f22f-dirty", "provisioner": "default", "id": "i-03691dff7f7c5089b", "hostname": "ip-192-168-95-67.ap-northeast-2.compute.internal", "instance-type": "m5.xlarge", "zone": "ap-northeast-2c", "capacity-type": "on-demand", "capacity": {"cpu":"4","ephemeral-storage":"20Gi","memory":"15155Mi","pods":"58"}}
2023-05-22T11:52:49.588Z	INFO	controller.provisioner.cloudprovider	launched instance	{"commit": "698f22f-dirty", "provisioner": "default", "id": "i-0dd5ad429b8a4fb1c", "hostname": "ip-192-168-70-7.ap-northeast-2.compute.internal", "instance-type": "m5.xlarge", "zone": "ap-northeast-2c", "capacity-type": "on-demand", "capacity": {"cpu":"4","ephemeral-storage":"20Gi","memory":"15155Mi","pods":"58"}}
2023-05-22T11:52:49.588Z	INFO	controller.provisioner.cloudprovider	launched instance	{"commit": "698f22f-dirty", "provisioner": "default", "id": "i-0c0613125e339e499", "hostname": "ip-192-168-182-17.ap-northeast-2.compute.internal", "instance-type": "m5.xlarge", "zone": "ap-northeast-2c", "capacity-type": "on-demand", "capacity": {"cpu":"4","ephemeral-storage":"20Gi","memory":"15155Mi","pods":"58"}}
2023-05-22T11:52:49.588Z	INFO	controller.provisioner.cloudprovider	launched instance	{"commit": "698f22f-dirty", "provisioner": "default", "id": "i-08a1e375dc91770ff", "hostname": "ip-192-168-174-179.ap-northeast-2.compute.internal", "instance-type": "m5.xlarge", "zone": "ap-northeast-2c", "capacity-type": "on-demand", "capacity": {"cpu":"4","ephemeral-storage":"20Gi","memory":"15155Mi","pods":"58"}}
2023-05-22T11:53:04.005Z	DEBUG	controller	deleted launch template	{"commit": "698f22f-dirty", "launch-template": "karpenter.k8s.aws/16624063517551845275"}
2023-05-22T11:54:04.652Z	INFO	controller.deprovisioning	deprovisioning via consolidation delete, terminating 2 machines ip-192-168-182-17.ap-northeast-2.compute.internal/m5.xlarge/on-demand, ip-192-168-70-7.ap-northeast-2.compute.internal/m5.xlarge/on-demand	{"commit": "698f22f-dirty"}
2023-05-22T11:54:04.709Z	INFO	controller.termination	cordoned node	{"commit": "698f22f-dirty", "node": "ip-192-168-182-17.ap-northeast-2.compute.internal"}
2023-05-22T11:54:04.720Z	INFO	controller.termination	cordoned node	{"commit": "698f22f-dirty", "node": "ip-192-168-70-7.ap-northeast-2.compute.internal"}
2023-05-22T11:54:05.138Z	INFO	controller.termination	deleted node	{"commit": "698f22f-dirty", "node": "ip-192-168-182-17.ap-northeast-2.compute.internal"}
2023-05-22T11:54:05.140Z	INFO	controller.termination	deleted node	{"commit": "698f22f-dirty", "node": "ip-192-168-70-7.ap-northeast-2.compute.internal"}
```

ìŠ¤ì¼€ì¼ ë‹¤ìš´! â†’ 1ê°œë¡œ ì¤„ì„

```bash
$kubectl scale deployment inflate --replicas 1
deployment.apps/inflate scaled
```

ë¡œê·¸ í™•ì¸

```bash
$kubectl logs -f -n karpenter -l app.kubernetes.io/name=karpenter -c controller
2023-05-22T11:52:49.588Z	INFO	controller.provisioner.cloudprovider	launched instance	{"commit": "698f22f-dirty", "provisioner": "default", "id": "i-03691dff7f7c5089b", "hostname": "ip-192-168-95-67.ap-northeast-2.compute.internal", "instance-type": "m5.xlarge", "zone": "ap-northeast-2c", "capacity-type": "on-demand", "capacity": {"cpu":"4","ephemeral-storage":"20Gi","memory":"15155Mi","pods":"58"}}
2023-05-22T11:52:49.588Z	INFO	controller.provisioner.cloudprovider	launched instance	{"commit": "698f22f-dirty", "provisioner": "default", "id": "i-0dd5ad429b8a4fb1c", "hostname": "ip-192-168-70-7.ap-northeast-2.compute.internal", "instance-type": "m5.xlarge", "zone": "ap-northeast-2c", "capacity-type": "on-demand", "capacity": {"cpu":"4","ephemeral-storage":"20Gi","memory":"15155Mi","pods":"58"}}
2023-05-22T11:52:49.588Z	INFO	controller.provisioner.cloudprovider	launched instance	{"commit": "698f22f-dirty", "provisioner": "default", "id": "i-0c0613125e339e499", "hostname": "ip-192-168-182-17.ap-northeast-2.compute.internal", "instance-type": "m5.xlarge", "zone": "ap-northeast-2c", "capacity-type": "on-demand", "capacity": {"cpu":"4","ephemeral-storage":"20Gi","memory":"15155Mi","pods":"58"}}
2023-05-22T11:52:49.588Z	INFO	controller.provisioner.cloudprovider	launched instance	{"commit": "698f22f-dirty", "provisioner": "default", "id": "i-08a1e375dc91770ff", "hostname": "ip-192-168-174-179.ap-northeast-2.compute.internal", "instance-type": "m5.xlarge", "zone": "ap-northeast-2c", "capacity-type": "on-demand", "capacity": {"cpu":"4","ephemeral-storage":"20Gi","memory":"15155Mi","pods":"58"}}
2023-05-22T11:53:04.005Z	DEBUG	controller	deleted launch template	{"commit": "698f22f-dirty", "launch-template": "karpenter.k8s.aws/16624063517551845275"}
2023-05-22T11:54:04.652Z	INFO	controller.deprovisioning	deprovisioning via consolidation delete, terminating 2 machines ip-192-168-182-17.ap-northeast-2.compute.internal/m5.xlarge/on-demand, ip-192-168-70-7.ap-northeast-2.compute.internal/m5.xlarge/on-demand	{"commit": "698f22f-dirty"}
2023-05-22T11:54:04.709Z	INFO	controller.termination	cordoned node	{"commit": "698f22f-dirty", "node": "ip-192-168-182-17.ap-northeast-2.compute.internal"}
2023-05-22T11:54:04.720Z	INFO	controller.termination	cordoned node	{"commit": "698f22f-dirty", "node": "ip-192-168-70-7.ap-northeast-2.compute.internal"}
2023-05-22T11:54:05.138Z	INFO	controller.termination	deleted node	{"commit": "698f22f-dirty", "node": "ip-192-168-182-17.ap-northeast-2.compute.internal"}
2023-05-22T11:54:05.140Z	INFO	controller.termination	deleted node	{"commit": "698f22f-dirty", "node": "ip-192-168-70-7.ap-northeast-2.compute.internal"}
2023-05-22T11:43:03.998Z	DEBUG	controller	discovered kube dns	{"commit": "698f22f-dirty", "kube-dns-ip": "10.100.0.10"}
2023-05-22T11:43:03.999Z	DEBUG	controller	discovered version	{"commit": "698f22f-dirty", "version": "v0.27.5"}
2023/05/22 11:43:03 Registering 2 clients
2023/05/22 11:43:03 Registering 2 informer factories
2023/05/22 11:43:03 Registering 3 informers
2023/05/22 11:43:03 Registering 5 controllers
2023-05-22T11:43:04.000Z	INFO	controller	Starting server	{"commit": "698f22f-dirty", "path": "/metrics", "kind": "metrics", "addr": "[::]:8080"}
2023-05-22T11:43:04.000Z	INFO	controller	Starting server	{"commit": "698f22f-dirty", "kind": "health probe", "addr": "[::]:8081"}
I0522 11:43:04.101075       1 leaderelection.go:248] attempting to acquire leader lease karpenter/karpenter-leader-election...
2023-05-22T11:43:04.189Z	INFO	controller	Starting informers...	{"commit": "698f22f-dirty"}
2023-05-22T11:54:31.756Z	INFO	controller.deprovisioning	deprovisioning via consolidation delete, terminating 1 machines ip-192-168-174-179.ap-northeast-2.compute.internal/m5.xlarge/on-demand	{"commit": "698f22f-dirty"}
2023-05-22T11:54:31.811Z	INFO	controller.termination	cordoned node	{"commit": "698f22f-dirty", "node": "ip-192-168-174-179.ap-northeast-2.compute.internal"}
2023-05-22T11:54:32.143Z	INFO	controller.termination	deleted node	{"commit": "698f22f-dirty", "node": "ip-192-168-174-179.ap-northeast-2.compute.internal"}

$kubectl get node -l type=karpenter
NAME                                               STATUS   ROLES    AGE    VERSION
ip-192-168-95-67.ap-northeast-2.compute.internal   Ready    <none>   109s   v1.24.13-eks-0a21954
$kubectl get node --label-columns=eks.amazonaws.com/capacityType,karpenter.sh/capacity-type
NAME                                                STATUS   ROLES    AGE    VERSION                CAPACITYTYPE   CAPACITY-TYPE
ip-192-168-30-154.ap-northeast-2.compute.internal   Ready    <none>   108m   v1.24.13-eks-0a21954   ON_DEMAND
ip-192-168-86-220.ap-northeast-2.compute.internal   Ready    <none>   108m   v1.24.13-eks-0a21954   ON_DEMAND
ip-192-168-95-67.ap-northeast-2.compute.internal    Ready    <none>   114s   v1.24.13-eks-0a21954                  on-demand
$kubectl get node --label-columns=node.kubernetes.io/instance-type,topology.kubernetes.io/zone
NAME                                                STATUS                     ROLES    AGE     VERSION                INSTANCE-TYPE   ZONE
ip-192-168-116-39.ap-northeast-2.compute.internal   Unknown                    <none>   11s                            c5.large        ap-northeast-2a
ip-192-168-30-154.ap-northeast-2.compute.internal   Ready                      <none>   109m    v1.24.13-eks-0a21954   m5.large        ap-northeast-2a
ip-192-168-86-220.ap-northeast-2.compute.internal   Ready                      <none>   109m    v1.24.13-eks-0a21954   m5.large        ap-northeast-2c
ip-192-168-95-67.ap-northeast-2.compute.internal    Ready,SchedulingDisabled   <none>   2m12s   v1.24.13-eks-0a21954   m5.xlarge       ap-northeast-2c

$kubectl delete deployment inflate
deployment.apps "inflate" deleted

$kubectl delete svc -n monitoring grafana
service "grafana" deleted
$helm uninstall karpenter --namespace karpenter
release "karpenter" uninstalled

$aws ec2 describe-launch-templates --filters Name=tag:karpenter.k8s.aws/cluster,Values=${CLUSTER_NAME} |
>     jq -r ".LaunchTemplates[].LaunchTemplateName" |
>     xargs -I{} aws ec2 delete-launch-template --launch-template-name {}
{
    "LaunchTemplate": {
        "LaunchTemplateId": "lt-0bdf1b82218c3ff67",
        "LaunchTemplateName": "karpenter.k8s.aws/10691513453991989385",
        "CreateTime": "2023-05-22T11:52:47+00:00",
        "CreatedBy": "arn:aws:sts::871103481195:assumed-role/myeks2-karpenter/1684755783702281349",
        "DefaultVersionNumber": 1,
        "LatestVersionNumber": 1
    }
}

$eksctl delete cluster --name "${CLUSTER_NAME}"
2023-05-22 20:56:27 [â„¹]  deleting EKS cluster "myeks2"
2023-05-22 20:56:28 [â„¹]  will drain 0 unmanaged nodegroup(s) in cluster "myeks2"
2023-05-22 20:56:28 [â„¹]  starting parallel draining, max in-flight of 1

```

ì•„ë˜ëŠ” ë°°í¬ëœ ìì›ì„ ì‚­ì œí•œ í›„ì˜ ìŠ¤í¬ë¦° ìƒ·ì´ë‹¤. ë¡œê·¸ì™€ EKS-NODE-VIEWë¥¼ í†µí•´ ê´€ë ¨ ë‚´ìš©ì„ í™•ì¸í•˜ë©´ ëœë‹¤. 


![](https://velog.velcdn.com/images/han-0315/post/a70b04b3-3f5d-40e4-ace9-112a5e553232/image.png)

